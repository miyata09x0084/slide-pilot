# LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavily Ã— Slidev
# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆPDF/YouTube/ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰
# ãƒ•ãƒ­ãƒ¼: æƒ…å ±åé›† -> ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º -> ç›®æ¬¡ç”Ÿæˆ -> ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ -> è©•ä¾¡ -> ä¿å­˜

# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import os
import re
import json
import shutil
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, Dict, Any, Union, List

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from typing_extensions import TypedDict
from langsmith import traceable
from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from app.config import settings
from app.core.config import TAVILY_API_KEY, SLIDE_FORMAT, MARP_THEME, MARP_PAGINATE
from app.core.llm import llm
from app.core.supabase import save_slide_to_supabase
from app.core.utils import (
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
    _log, _strip_bullets, _slugify_en, _find_json,
    _ensure_marp_header, _insert_separators, _double_separators,
    _clean_title, _strip_whole_code_fence, _remove_presenter_lines,
    # æ—¥æ™‚é–¢æ•°
    JST, now_jst, today_iso, month_ja, month_en, get_today_jst,
    # Slidevç”Ÿæˆé–¢æ•°
    _get_all_vendors_info, _create_llm_summarized_bullets,
    _generate_multi_vendor_slides_integrated,
    # Tavilyæ¤œç´¢é–¢æ•°
    tavily_search, tavily_collect_context, context_to_bullets,
    # ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
    generate_slidev_test,
)
from app.prompts.evaluation_prompts import get_evaluation_prompt
from app.prompts.slide_prompts import (
    get_key_points_map_prompt,
    get_key_points_reduce_prompt,
    get_key_points_ai_prompt,
    get_toc_pdf_prompt,
    get_toc_ai_prompt,
    get_slide_title_prompt,
    get_slide_pdf_prompt,
    get_slug_prompt,
)
from app.tools.pdf import process_pdf


# -------------------
# State
# -------------------
class State(TypedDict, total=False):
  """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®çŠ¶æ…‹ç®¡ç†

  NOTE: user_id ã¯å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆRead-Onlyï¼‰ã§ã‚ã‚Šã€
        ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªã„ã€‚ãƒãƒ¼ãƒ‰å†…ã§å¤‰æ›´ç¦æ­¢ã€‚
        å°†æ¥ LangGraph v0.6+ ã§ã¯ context_schema ã«ç§»è¡Œäºˆå®šã€‚

  total=False ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã¨ã—ã€
  å¾Œæ–¹äº’æ›æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹ã€‚
  """

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆRead-Onlyã€ãƒãƒ¼ãƒ‰ã§å¤‰æ›´ç¦æ­¢ï¼‰
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  user_id: str                                  # ãƒ¦ãƒ¼ã‚¶ãƒ¼è­˜åˆ¥å­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "anonymous"ï¼‰

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å…¥åŠ›
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  topic: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä¸»é¡Œ

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # æƒ…å ±åé›† (Node A)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  sources: Dict[str, List[Dict[str, str]]]      # Tavilyæ¤œç´¢çµæœ
  context_md: str                               # æ¤œç´¢çµæœã®Markdown

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ (Node B-D)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  key_points: List[str]                         # é‡è¦ãƒã‚¤ãƒ³ãƒˆ5å€‹
  toc: List[str]                                # ç›®æ¬¡5-8é …ç›®
  slide_md: str                                 # Marpã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡
  title: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # è©•ä¾¡ (Node E)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  score: float                                  # ç·åˆã‚¹ã‚³ã‚¢ (0-10)
  subscores: Dict[str, float]                   # é …ç›®åˆ¥ã‚¹ã‚³ã‚¢
  reasons: Dict[str, str]                       # è©•ä¾¡ç†ç”±
  suggestions: List[str]                        # æ”¹å–„ææ¡ˆ
  risk_flags: List[str]                         # ãƒªã‚¹ã‚¯äº‹é …
  passed: bool                                  # åˆæ ¼åˆ¤å®š (>=8.0)
  feedback: str                                 # ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
  attempts: int                                 # ãƒªãƒˆãƒ©ã‚¤å›æ•° (æœ€å¤§3)

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å›³è§£ç”Ÿæˆ (Node D.5) - Issue #25
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  diagrams: Dict[str, Any]                      # ç”Ÿæˆã•ã‚ŒãŸå›³è§£ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å‡ºåŠ› (Node F)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  slide_path: str                               # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
  slide_id: str                                 # Supabase slide IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
  pdf_url: str                                  # Supabaseå…¬é–‹URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ã‚·ã‚¹ãƒ†ãƒ 
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  error: str                                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  log: List[str]                                # å®Ÿè¡Œãƒ­ã‚°

# =======================
# å…¥åŠ›ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤åˆ¥
# =======================
def detect_input_type(topic: str) -> str:
    """
    å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    Returns: "pdf" | "youtube" | "text"
    """
    topic_lower = topic.lower()

    # YouTube URLãƒ‘ã‚¿ãƒ¼ãƒ³
    youtube_patterns = [
        r'youtube\.com/watch\?v=',
        r'youtu\.be/',
        r'youtube\.com/embed/',
    ]
    for pattern in youtube_patterns:
        if re.search(pattern, topic_lower):
            return "youtube"

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
    if topic_lower.endswith('.pdf') or '/uploads/' in topic_lower:
        return "pdf"

    # ãã‚Œä»¥å¤–ã¯ãƒ†ã‚­ã‚¹ãƒˆï¼ˆTavilyæ¤œç´¢ï¼‰
    return "text"

# =======================
# Node A: æƒ…å ±åé›†ï¼ˆPDF/YouTube/Tavilyå¯¾å¿œï¼‰
# =======================
@traceable(run_name="a_collect_info")
def collect_info(state: State) -> State:
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    if input_type == "pdf":
      result = process_pdf(topic)
      data = json.loads(result)

      if data["status"] == "success":
        # PDFã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´å½¢ï¼ˆâ˜…å…¨æ–‡ä¿æŒã«å¤‰æ›´ï¼‰
        pdf_filename = Path(topic).stem
        full_content = data['content']  # ãƒãƒ£ãƒ³ã‚¯åŒºåˆ‡ã‚Š "---" ã§çµåˆæ¸ˆã¿
        context_md = f"# PDF: {pdf_filename}\n\n{full_content}"

        # sourcesã«ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
        chunks = full_content.split("\n\n---\n\n")
        sources = {
          "pdf_content": [{
            "title": pdf_filename,
            "url": topic,
            "content": data['content'][:500],  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨
            "num_pages": data.get('num_pages', 0),
            "total_chars": data.get('total_chars', 0),
            "num_chunks": len(chunks),
            "chunks": chunks  # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿æŒ
          }]
        }

        return {
          "sources": sources,
          "context_md": context_md,
          "log": _log(state, f"[pdf] pages={data.get('num_pages')}, chars={data.get('total_chars')}, chunks={len(chunks)}")
        }
      else:
        return {"error": f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {data['message']}", "log": _log(state, f"[pdf] ERROR {data['message']}")}

    # YouTubeå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
    elif input_type == "youtube":
      return {"error": "YouTubeå‡¦ç†ã¯æº–å‚™ä¸­ã§ã™ï¼ˆIssue #18ã§å®Ÿè£…äºˆå®šï¼‰", "log": _log(state, "[youtube] NOT_IMPLEMENTED")}

    # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ï¼ˆæ—¢å­˜ã®Tavilyæ¤œç´¢ï¼‰
    else:
      # JSTã®ç¾åœ¨æ—¥æ™‚ ã¨ã€€æœˆè‹±èªè¡¨è¨˜ã‚’å–å¾—
      def month_en_for(dt: datetime) -> str:
        months = ["January","February","March","April","May","June",
                  "July","August","September","October","November","December"]
        return f"{months[dt.month-1]} {dt.year}"

      now = now_jst()
      # å…ˆæœˆã¯ã€æœˆæœ«æ—¥ã®1æ—¥ã«ãªã‚‹
      prev_month_dt = now.replace(day=1) - timedelta(days=1)
      # ç›´è¿‘2ãƒ¶æœˆ
      month_labels = [month_en_for(now), month_en_for(prev_month_dt)]

      # ãƒ™ãƒ³ãƒ€ãƒ¼æ¯ã®å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³
      vendors_domains = [
            (["azure.microsoft.com","news.microsoft.com","learn.microsoft.com"],
             ["Microsoft AI updates", "Azure OpenAI updates"]),
            (["openai.com"], ["OpenAI announcements","OpenAI updates"]),
            (["blog.google","ai.googleblog.com","research.google"], ["Google AI updates", "Gemini updates"]),
            (["aws.amazon.com"], ["AWS Bedrock updates","Amazon AI updates"]),
            (["ai.meta.com"], ["Meta AI updates", "Llama updates"]),
            (["anthropic.com"], ["Anthropic Claude updates","Claude announcements"]),
        ]

      # â˜… ç›´è¿‘2ãƒ¶æœˆ Ã— å„ç¤¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹æˆ
      queries: List[Dict[str, Any]] = []
      for m in month_labels:
        for domains, patterns in vendors_domains:
          for p in patterns:
            queries.append({"q": f"{p} {m}", "include_domains": domains, "time_range": "month"})

      sources = tavily_collect_context(queries, max_per_query=6, default_time_range="month")
      context_md = context_to_bullets(sources)
      return {
        "sources": sources,
        "context_md": context_md,
        "log": _log(state, f"[tavily] months={month_labels} queries={len(queries)}")
      }

  except Exception as e:
    return {"error": f"collect_info_error: {e}", "log": _log(state, f"[collect_info] EXCEPTION {e}")}

# -------------------
# Node B: é‡è¦ãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
# -------------------
@traceable(run_name="b_generate_key_points")
def generate_key_points(state: State) -> Dict:
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"
  ctx = state.get("context_md") or ""
  sources = state.get("sources") or {}

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯Map-Reduceæ–¹å¼ã§å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
  if input_type == "pdf":
    try:
      # ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = ctx.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # Map: å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºï¼ˆæœ€å¤§3å€‹ï¼‰
      chunk_points = []
      for i, chunk in enumerate(chunks[:20]):  # æœ€å¤§20ãƒãƒ£ãƒ³ã‚¯ï¼ˆLLMã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
        if not chunk.strip():
          continue

        map_prompt = get_key_points_map_prompt(chunk=chunk, chunk_index=i+1)

        msg = llm.invoke(map_prompt)
        points = _strip_bullets(msg.content.splitlines())[:3]
        chunk_points.extend(points)

      # Reduce: å…¨ãƒã‚¤ãƒ³ãƒˆã‚’çµ±åˆã—ã¦5ã¤ã«å‡ç¸®
      if chunk_points:
        reduce_prompt = get_key_points_reduce_prompt(chunk_points=chunk_points)

        msg = llm.invoke(reduce_prompt)
        lines = msg.content.splitlines()

        # å‰ç½®ãè¡Œã‚’é™¤å¤–ï¼ˆç®‡æ¡æ›¸ãè¨˜å·ã¾ãŸã¯ç•ªå·ã§å§‹ã¾ã‚‹è¡Œã®ã¿æŠ½å‡ºï¼‰
        filtered_lines = [
          line for line in lines
          if line.strip() and (
            line.strip().startswith(('-', 'â€¢', '*', 'ãƒ»')) or
            re.match(r'^\d+\.', line.strip())
          )
        ]

        final_bullets = _strip_bullets(filtered_lines)[:5] if filtered_lines else chunk_points[:5]

        # 5å€‹æœªæº€ã®å ´åˆã¯chunk_pointsã‹ã‚‰è£œå……
        while len(final_bullets) < 5 and chunk_points:
          candidate = chunk_points.pop(0)
          if candidate not in final_bullets:  # é‡è¤‡å›é¿
            final_bullets.append(candidate)
      else:
        final_bullets = ["å†…å®¹ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"]

      return {
        "key_points": final_bullets,
        "log": _log(state, f"[key_points_map_reduce] chunks={len(chunks)}, extracted={len(chunk_points)}, final={len(final_bullets)}")
      }

    except Exception as e:
      return {"error": f"key_points_pdf_error: {e}", "log": _log(state, f"[key_points_pdf] EXCEPTION {e}")}

  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = get_key_points_ai_prompt(context_md=ctx, topic=topic)

    try:
      msg = llm.invoke(prompt)
      bullets = _strip_bullets(msg.content.splitlines())[:5] or [msg.content.strip()]
      return {"key_points": bullets, "log": _log(state, f"[key_points] {bullets}")}
    except Exception as e:
      return {"error": f"key_points_error: {e}", "log": _log(state, f"[key_points] EXCEPTION {e}")}

# -------------------
# Node C: ç›®æ¬¡ç”Ÿæˆ
# -------------------
@traceable(run_name="c_generate_toc")
def generate_toc(state: State) -> Dict:
  topic = state.get("topic") or ""
  key_points = state.get("key_points") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯ä¸­å­¦ç”Ÿå‘ã‘ã®ç« ç«‹ã¦
  if input_type == "pdf":
    prompt = get_toc_pdf_prompt(key_points=key_points)
  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = get_toc_ai_prompt(key_points=key_points)

  try:
    msg = llm.invoke(prompt)
    try:
      data = json.loads(_find_json(msg.content) or msg.content)
      toc = [s.strip() for s in data.get("toc", []) if s.strip()]
    except Exception as e:
      toc = _strip_bullets(msg.content.splitlines())
      toc = toc[:8] or ["ã¯ã˜ã‚ã«", "èƒŒæ™¯", "å®Ÿè£…æ‰‹é †", "è©•ä¾¡ã¨æ”¹å–„", "å…¬é–‹ãƒ»é‹ç”¨", "ã¾ã¨ã‚"]
    return {"toc": toc, "error": "", "log": _log(state, f"[toc] {toc}")}
  except Exception as e:
    return {"error": f"toc_error: {e}", "log": _log(state, f"[toc] EXCEPTION {e}")}

# -------------------
# Mermaidå›³è§£ç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆIssue #25ï¼‰
# -------------------
# ä»¥ä¸‹ã®é–¢æ•°ã¯å»ƒæ­¢ï¼ˆLLMãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‹¬è‡ªã®å›³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ä¸è¦ï¼‰
# def _generate_architecture_flowchart(key_points: List[str]) -> str:
# def _generate_use_case_mindmap(key_points: List[str]) -> str:


def _insert_after_section(slide_md: str, section_title: str, content: str) -> str:
    """æŒ‡å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ç›´å¾Œã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŒ¿å…¥ï¼ˆh1/h2/h3å¯¾å¿œï¼‰"""
    import re

    # "# section_title" ã¾ãŸã¯ "## section_title" ã®å¾Œã® "---" ã‚’è¦‹ã¤ã‘ã¦æŒ¿å…¥
    # contentã®å…ˆé ­ã¨æœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ã€åŒºåˆ‡ã‚Šã‚’è¿½åŠ ã—ã¦æŒ¿å…¥
    clean_content = content.strip('\n')
    pattern = rf'(#+\s+{re.escape(section_title)}.*?\n---\s*\n)'

    if re.search(pattern, slide_md, re.DOTALL):
        return re.sub(pattern, rf'\1\n{clean_content}\n\n---\n\n', slide_md, count=1, flags=re.DOTALL)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›®æ¬¡/Agendaã®å¾Œã«æŒ¿å…¥
        agenda_pattern = r'(#+\s+(?:ç›®æ¬¡|Agenda).*?\n---\s*\n)'
        if re.search(agenda_pattern, slide_md, re.DOTALL):
            return re.sub(agenda_pattern, rf'\1\n{clean_content}\n\n---\n\n', slide_md, count=1, flags=re.DOTALL)
        return slide_md


def _insert_before_section(slide_md: str, section_title: str, content: str) -> str:
    """æŒ‡å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ç›´å‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŒ¿å…¥ï¼ˆh1/h2/h3å¯¾å¿œï¼‰"""
    import re

    # contentã®å…ˆé ­ã¨æœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
    clean_content = content.strip('\n')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³: --- ã®å¾Œã« section_title ãŒã‚ã‚‹ç®‡æ‰€
    # ãƒãƒƒãƒã‚°ãƒ«ãƒ¼ãƒ—1: --- + æ”¹è¡Œã€ã‚°ãƒ«ãƒ¼ãƒ—2: section_title
    pattern = rf'(---\s*\n\n)(#+\s+{re.escape(section_title)})'

    if re.search(pattern, slide_md):
        # --- ã¨ section_title ã®é–“ã«å›³è§£ã‚’æŒ¿å…¥
        return re.sub(pattern, rf'\1{clean_content}\n\n---\n\n\2', slide_md, count=1)
    else:
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ«å°¾ã«è¿½åŠ 
        return slide_md.rstrip('\n') + f'\n\n{clean_content}\n\n---\n\n'

# -------------------
# Node D: ã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡ï¼ˆSlidevï¼‰ç”Ÿæˆ
# -------------------
@traceable(run_name="d_generate_slide_slidev")
def write_slides_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆï¼ˆå…¨6ç¤¾å¯¾å¿œ / PDFå¯¾å¿œï¼‰"""
  sources = state.get("sources") or {}
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"
  context_md = state.get("context_md") or ""
  key_points = state.get("key_points") or []
  toc = state.get("toc") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ã®å ´åˆã¯æ±ç”¨çš„ãªã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    if input_type == "pdf":
      # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¦ç´„ã‚’ä½œæˆã—ã¦ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = context_md.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # PDFã®å†…å®¹ã‹ã‚‰LLMã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
      title_prompt = get_slide_title_prompt(chunks=chunks, key_points=key_points)

      try:
        title_msg = llm.invoke(title_prompt)
        ja_title = title_msg.content.strip().replace('"', '').replace("'", '')
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(ja_title) > 30:
          ja_title = ja_title[:30] + "..."
      except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
        pdf_filename = Path(topic).stem if topic.endswith('.pdf') else "PDFè³‡æ–™"
        # UUIDã‚’é™¤å»
        if '_' in pdf_filename:
          pdf_filename = pdf_filename.split('_', 1)[1] if len(pdf_filename.split('_', 1)) > 1 else pdf_filename
        ja_title = f"{pdf_filename}"

      # ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆè¦ç´„ãªã—ãƒ»ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‰ˆï¼‰
      chunk_texts = []
      total_chars = 0
      for i, chunk in enumerate(chunks[:20]):
        if not chunk.strip():
          continue

        # å„ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­1500æ–‡å­—ã‚’ä½¿ç”¨
        chunk_preview = chunk[:1500]
        chunk_texts.append(f"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i+1}\n{chunk_preview}")
        total_chars += len(chunk_preview)

        # åˆè¨ˆ15,000æ–‡å­—ã¾ã§ä½¿ç”¨ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã®ä¸Šé™ï¼‰
        if total_chars > 15000:
          break

      # å…¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
      full_summary = "\n\n".join(chunk_texts)

      # LLMã§Slidevãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯æŠœç²‹ç‰ˆã‚’ä½¿ç”¨ï¼‰
      prompt = get_slide_pdf_prompt(
        full_summary=full_summary,
        key_points=key_points,
        toc=toc,
        ja_title=ja_title
      )

      msg = llm.invoke(prompt)
      slide_md = msg.content.strip()

      # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚Œã°é™¤å»
      slide_md = _strip_whole_code_fence(slide_md)

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev_pdf] generated ({len(slide_md)} chars) from {len(chunk_texts)} chunks (cost-optimized, no LLM summary)")
      }

    # AIæœ€æ–°æƒ…å ±ï¼ˆTavilyï¼‰ã®å ´åˆã¯æ—¢å­˜ã®ãƒãƒ«ãƒãƒ™ãƒ³ãƒ€ãƒ¼ç”Ÿæˆ
    else:
      ja_title = f"{month_ja()} AIæœ€æ–°æƒ…å ±ã¾ã¨ã‚"
      slide_md = _generate_multi_vendor_slides_integrated(
        topic=ja_title,
        sources=sources,
        mvp_version="AI Industry Report 2025"
      )

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev] generated ({len(slide_md)} chars, 6 vendors)")
      }

  except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    fallback_md = f"""---
theme: apple-basic
highlighter: shiki
class: text-center
---

# ğŸš€ {ja_title}
## ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ

<div class="pt-12">
  <span class="px-2 py-1 rounded" style="background: #ef4444; color: white;">
    Error: {str(e)}
  </span>
</div>

---

## âš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°

ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

- æ¤œç´¢çµæœã®å–å¾—ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„

"""
    return {
      "slide_md": fallback_md,
      "title": ja_title,
      "error": f"slides_slidev_error: {e}",
      "log": _log(state, f"[slides_slidev] EXCEPTION {e} - using fallback")
    }

# -------------------
# Node D.5: Mermaidå›³è§£ç”Ÿæˆï¼ˆIssue #25ï¼‰
# -------------------
@traceable(run_name="d5_generate_diagrams")
# generate_diagrams ãƒãƒ¼ãƒ‰ã¯å»ƒæ­¢ï¼ˆLLMãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‹¬è‡ªã®å›³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ä¸è¦ï¼‰
# Issue #25: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå›³ã®å¼·åˆ¶æŒ¿å…¥ã‚’å‰Šé™¤ã—ã€LLMã«ã‚ˆã‚‹ç‹¬è‡ªå›³ç”Ÿæˆã«ç§»è¡Œ
def generate_diagrams(state: State) -> Dict:
    """[DEPRECATED] ã“ã®ãƒãƒ¼ãƒ‰ã¯ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“"""
    return {"log": _log(state, "[diagrams] deprecated - skipped")}

# -------------------
# Node E: è©•ä¾¡
# -------------------
MAX_ATTEMPTS = 3

# Slidevç”¨è©•ä¾¡ãƒãƒ¼ãƒ‰
@traceable(run_name="e_evaluate_slides_slidev")
def evaluate_slides_slidev(state: State) -> Dict:
  """Slidevã‚¹ãƒ©ã‚¤ãƒ‰ã®å“è³ªè©•ä¾¡ï¼ˆPDF/AIæƒ…å ±å¯¾å¿œï¼‰"""
  if state.get("error"):
    return {}
  slide_md = state.get("slide_md") or ""
  toc = state.get("toc") or []
  topic = state.get("topic") or ""

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã—ã¦PDFç‰¹æœ‰ã®è©•ä¾¡åŸºæº–ã‚’è¿½åŠ 
  input_type = detect_input_type(topic)

  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆå…¥åŠ›ã‚¿ã‚¤ãƒ—ã§è©•ä¾¡åŸºæº–ã‚’åˆ‡ã‚Šæ›¿ãˆï¼‰
  prompt = get_evaluation_prompt(
    slide_md=slide_md,
    toc=toc,
    topic=topic,
    input_type=input_type
  )
  try:
    msg = llm.invoke(prompt)
    raw = msg.content or ""
    js = _find_json(raw) or raw
    data = json.loads(js)

    score = float(data.get("score", 0.0))
    subscores = data.get("subscores") or {}
    reasons = data.get("reasons") or {}
    suggestions = data.get("suggestions") or []
    risk_flags = data.get("risk_flags") or []
    passed = bool(data.get("pass", score >= 8.0))
    feedback = str(data.get("feedback", "")).strip()
    attempts = (state.get("attempts") or 0) + 1

    return {
      "score": score,
      "subscores": subscores,
      "reasons": reasons,
      "suggestions": suggestions,
      "risk_flags": risk_flags,
      "passed": passed,
      "feedback": feedback,
      "attempts": attempts,
      "log": _log(state, f"[evaluate_slidev] score={score:.2f} pass={passed} attempts={attempts}")
    }
  except Exception as e:
    return {"error": f"eval_error: {e}", "log": _log(state, f"[evaluate_slidev] EXCEPTION {e}")}

def route_after_eval_slidev(state: State) -> str:
    """è©•ä¾¡çµæœã«åŸºã¥ã„ã¦ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯å®Œäº†ã‚’åˆ¤å®š"""
    if (state.get("attempts") or 0) >= MAX_ATTEMPTS:
        return "ok"
    return "ok" if state.get("passed") else "retry"

# -------------------
# Node F: ä¿å­˜ & Slidevãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
# -------------------
@traceable(run_name="f_save_and_render_slidev")
def save_and_render_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä¿å­˜ã—ã¦PDFç”Ÿæˆ"""
  if state.get("error"):
    return {}

  slide_md = state.get("slide_md") or ""
  title = state.get("title") or "AIã‚¹ãƒ©ã‚¤ãƒ‰"

  # ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
  if not slide_md.strip():
    return {
      "error": "slide_md is empty",
      "log": _log(state, "[save_slidev] ERROR: slide_md is empty")
    }

  # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«åã®è‹±èªè¡¨è¨˜ã‚’ç”Ÿæˆ
  slug_prompt = get_slug_prompt(title=title)

  try:
    emsg = llm.invoke(slug_prompt)
    file_stem = _slugify_en(emsg.content.strip()) or _slugify_en(title)
  except Exception:
    file_stem = _slugify_en(title) or "ai-latest-info"

  # çµ±ä¸€è¨­å®šã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
  slide_dir = settings.SLIDES_DIR
  slide_md_path = slide_dir / f"{file_stem}_slidev.md"
  slide_md_path.write_text(slide_md, encoding="utf-8")

  # Slidev PDFç”Ÿæˆ
  slidev = shutil.which("slidev")
  out_path = str(slide_md_path)

  if slidev and SLIDE_FORMAT == "pdf":
    pdf_file = slide_dir / f"{file_stem}_slidev.pdf"
    try:
      subprocess.run(
        ["slidev", "export", str(slide_md_path),
         "--output", str(pdf_file),
         "--format", "pdf",
         "--timeout", "60000"],  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        check=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        timeout=90  # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
      )
      out_path = str(pdf_file)
      log_msg = f"[slidev] generated PDF -> {pdf_file.name}"
    except subprocess.TimeoutExpired:
      log_msg = f"[slidev] PDF generation timeout (90s) - MD saved at {slide_md_path.name}"
    except subprocess.CalledProcessError as e:
      error_msg = e.stderr.decode() if e.stderr else str(e)
      log_msg = f"[slidev] export failed: {error_msg[:100]} - MD saved"
  else:
    if not slidev:
      log_msg = "[slidev] slidev-cli not found â€“ skipped rendering (left .md)."
    else:
      log_msg = f"[slidev] rendering skipped (SLIDE_FORMAT={SLIDE_FORMAT}, only pdf supported)."

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Supabaseä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã€å¤±æ•—ã—ã¦ã‚‚ç¶™ç¶šï¼‰
  # Issue #24: ãƒ–ãƒ©ã‚¦ã‚¶ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ + Supabaseå±¥æ­´ç®¡ç†
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  result = {
    "slide_path": out_path,
    "log": _log(state, log_msg)
  }

  # å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰user_idã‚’å–å¾—ï¼ˆRead-Onlyï¼‰
  user_id = state.get("user_id", "anonymous")
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")

  try:
    # PDFãŒç”Ÿæˆã•ã‚ŒãŸå ´åˆã®ã¿ãƒ‘ã‚¹ã‚’æ¸¡ã™
    pdf_path = None
    if slidev and SLIDE_FORMAT == "pdf" and "pdf_file" in locals():
      pdf_path = pdf_file

    supabase_result = save_slide_to_supabase(
      user_id=user_id,
      title=title,
      topic=topic,
      slide_md=slide_md,
      pdf_path=pdf_path
    )

    if "slide_id" in supabase_result:
      result["slide_id"] = supabase_result["slide_id"]
      result["pdf_url"] = supabase_result.get("pdf_url")
      result["log"] = _log(state, f"[supabase] saved slide_id={supabase_result['slide_id']}")
    elif "error" in supabase_result:
      # Supabaseæœªè¨­å®šã®å ´åˆã‚‚ã“ã“ã«å…¥ã‚‹ï¼ˆè­¦å‘Šã®ã¿ã€ç¶™ç¶šï¼‰
      result["log"] = _log(state, f"[supabase] {supabase_result['error']}")

  except Exception as e:
    # Supabaseä¿å­˜å¤±æ•—ã—ã¦ã‚‚ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã¯ç¶™ç¶šï¼ˆã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„ï¼‰
    result["log"] = _log(state, f"[supabase] save failed (non-critical): {str(e)[:100]}")

  return result

# -------------------
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# -------------------
graph_builder = StateGraph(State)
graph_builder.add_node("collect_info", collect_info)
graph_builder.add_node("generate_key_points", generate_key_points)
graph_builder.add_node("generate_toc", generate_toc)
graph_builder.add_node("write_slides_slidev", write_slides_slidev)
graph_builder.add_node("generate_diagrams", generate_diagrams)
graph_builder.add_node("save_and_render_slidev", save_and_render_slidev)
graph_builder.add_node("evaluate_slides_slidev", evaluate_slides_slidev)

# ã‚¨ãƒƒã‚¸å®šç¾©ï¼ˆSlidevãƒ•ãƒ­ãƒ¼ with è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼‰
graph_builder.add_edge(START, "collect_info")
graph_builder.add_edge("collect_info", "generate_key_points")
graph_builder.add_edge("generate_key_points", "generate_toc")
graph_builder.add_edge("generate_toc", "write_slides_slidev")
graph_builder.add_edge("write_slides_slidev", "evaluate_slides_slidev")

# è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
graph_builder.add_conditional_edges(
  "evaluate_slides_slidev",
  route_after_eval_slidev,
  {"retry": "generate_key_points", "ok": "save_and_render_slidev"}
)

graph_builder.add_edge("save_and_render_slidev", END)

graph = graph_builder.compile()

# -------------------
# å®Ÿè¡Œ
# -------------------
if __name__ == "__main__":
  print("LangSmith Tracing:", os.getenv("LANGCHAIN_TRACING_V2"),
      "| Project:", os.getenv("LANGCHAIN_PROJECT"))
  init: State = {
    "topic": "LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavilyã§ä½œã‚‹ï¼šæœ€æ–°AIå‹•å‘ã‚¹ãƒ©ã‚¤ãƒ‰",
    "key_points": [], "toc": [], "slide_md": "",
    "score": 0.0,
    "subscores": {}, "reasons": {},
    "suggestions": [], "risk_flags": [],
    "passed": False, "feedback": "",
    "title": "", "slide_path": "",
    "attempts": 0, "error": "", "log": [],
    "context_md": "", "sources": {}
  }

  config: RunnableConfig = {
    "run_name": "tavily_marp_agent",
    "tags": ["marp", "langgraph", "langsmith", "openai", "tavily"],
    "metadata": {"env": "dev", "date": datetime.now(timezone.utc).isoformat()},
    "recursive_limit": 60,
  }
  out = graph.invoke(init, config=config)

  print("\n=== RESULT ===")
  if out.get("error"):
    print("ERROR:", out["error"])
  else:
    print("Title    :", out.get("title"))
    print("Slide    :", out.get("slide_path"))
    # print("Score    :", out.get("score"))
    # print("Passed   :", out.get("passed"))
  print("\n=== LOGS ===")
  for line in out.get("log", []):
    print(line)