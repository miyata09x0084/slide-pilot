# LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavily Ã— Slidev
# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆPDF/YouTube/ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰
# ãƒ•ãƒ­ãƒ¼: æƒ…å ±åé›† -> ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º -> ç›®æ¬¡ç”Ÿæˆ -> ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ -> è©•ä¾¡ -> ä¿å­˜

# æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import os
import re
import json
import shutil
import subprocess
import tempfile
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Optional, Dict, Any, Union, List

# ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from typing_extensions import TypedDict
from langsmith import traceable
from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from app.config import settings
from app.core.config import TAVILY_API_KEY
from app.core.llm import llm
from app.core.supabase import save_slide_to_supabase
from app.core.storage import upload_to_storage
from app.core.utils import (
    _log,
    _strip_bullets,
    _slugify_en,
    _find_json,
    _insert_separators,
    _double_separators,
    _strip_whole_code_fence,
    _format_conversation,
    JST,
    now_jst,
    month_ja,
    _get_all_vendors_info,
    _create_llm_summarized_bullets,
    _generate_multi_vendor_slides_integrated,
    tavily_search,
    tavily_collect_context,
    context_to_bullets,
)
from app.prompts.evaluation_prompts import get_evaluation_prompt
from app.prompts.slide_prompts import (
    get_key_points_map_prompt,
    get_key_points_reduce_prompt,
    get_key_points_ai_prompt,
    get_toc_pdf_prompt,
    get_toc_ai_prompt,
    get_slide_title_prompt,
    get_slide_pdf_prompt,
    get_slug_prompt,
)
from app.tools.pdf import process_pdf


# -------------------
# State
# -------------------
class State(TypedDict, total=False):
  """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®çŠ¶æ…‹ç®¡ç†

  NOTE: user_id ã¯å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆRead-Onlyï¼‰ã§ã‚ã‚Šã€
        ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªã„ã€‚ãƒãƒ¼ãƒ‰å†…ã§å¤‰æ›´ç¦æ­¢ã€‚
        å°†æ¥ LangGraph v0.6+ ã§ã¯ context_schema ã«ç§»è¡Œäºˆå®šã€‚

  total=False ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€å…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã¨ã—ã€
  å¾Œæ–¹äº’æ›æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã‚‹ã€‚
  """

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆRead-Onlyã€ãƒãƒ¼ãƒ‰ã§å¤‰æ›´ç¦æ­¢ï¼‰
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  user_id: str                                  # ãƒ¦ãƒ¼ã‚¶ãƒ¼è­˜åˆ¥å­ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: "anonymous"ï¼‰

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å…¥åŠ›
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  topic: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä¸»é¡Œ

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # æƒ…å ±åé›† (Node A)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  sources: Dict[str, List[Dict[str, str]]]      # Tavilyæ¤œç´¢çµæœ
  context_md: str                               # æ¤œç´¢çµæœã®Markdown

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ (Node B-D)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  key_points: List[str]                         # é‡è¦ãƒã‚¤ãƒ³ãƒˆ5å€‹
  toc: List[str]                                # ç›®æ¬¡5-8é …ç›®
  slide_md: str                                 # Marpã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡
  title: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # è©•ä¾¡ (Node E)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  score: float                                  # ç·åˆã‚¹ã‚³ã‚¢ (0-10)
  subscores: Dict[str, float]                   # é …ç›®åˆ¥ã‚¹ã‚³ã‚¢
  reasons: Dict[str, str]                       # è©•ä¾¡ç†ç”±
  suggestions: List[str]                        # æ”¹å–„ææ¡ˆ
  risk_flags: List[str]                         # ãƒªã‚¹ã‚¯äº‹é …
  passed: bool                                  # åˆæ ¼åˆ¤å®š (>=8.0)
  feedback: str                                 # ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
  attempts: int                                 # ãƒªãƒˆãƒ©ã‚¤å›æ•° (æœ€å¤§3)

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å›³è§£ç”Ÿæˆ (Node D.5) - Issue #25
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  diagrams: Dict[str, Any]                      # ç”Ÿæˆã•ã‚ŒãŸå›³è§£ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å‡ºåŠ› (Node F)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  slide_path: str                               # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
  slide_id: str                                 # Supabase slide IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
  pdf_url: str                                  # Supabaseå…¬é–‹URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # å‹•ç”»ç”Ÿæˆ (Node G-H) - Video Narration Feature
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  slides_json: List[Dict[str, Any]]             # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ‡ãƒ¼ã‚¿ï¼ˆHTMLç”Ÿæˆç”¨ï¼‰
  narration_scripts: List[str]                  # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å°æœ¬
  audio_files: List[str]                        # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
  video_url: str                                # Supabaseå‹•ç”»URLï¼ˆåŒæœŸç‰ˆã§ä½¿ç”¨ï¼‰
  video_job_id: str                             # Cloud Run Job IDï¼ˆéåŒæœŸç‰ˆã§ä½¿ç”¨ï¼‰
  _temp_narration_dir: str                      # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå†…éƒ¨ç”¨ï¼‰

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # ã‚·ã‚¹ãƒ†ãƒ 
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  error: str                                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  log: List[str]                                # å®Ÿè¡Œãƒ­ã‚°

# =======================
# å…¥åŠ›ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤åˆ¥
# =======================
def detect_input_type(topic: str) -> str:
    """
    å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    Returns: "pdf" | "youtube" | "text"
    """
    topic_lower = topic.lower()

    # YouTube URLãƒ‘ã‚¿ãƒ¼ãƒ³
    youtube_patterns = [
        r'youtube\.com/watch\?v=',
        r'youtu\.be/',
        r'youtube\.com/embed/',
    ]
    for pattern in youtube_patterns:
        if re.search(pattern, topic_lower):
            return "youtube"

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
    if topic_lower.endswith('.pdf') or '/uploads/' in topic_lower:
        return "pdf"

    # ãã‚Œä»¥å¤–ã¯ãƒ†ã‚­ã‚¹ãƒˆï¼ˆTavilyæ¤œç´¢ï¼‰
    return "text"

# =======================
# ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =======================
def extract_clean_title(raw_title: str, fallback: str = "è³‡æ–™") -> str:
    """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆèª¬æ˜æ–‡ãƒ»è¨˜å·é™¤å»ï¼‰

    Args:
        raw_title: LLMã®ç”Ÿã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        fallback: æŠ½å‡ºå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒˆãƒ«

    Returns:
        ã‚¯ãƒªãƒ¼ãƒ³ãªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ15æ–‡å­—ä»¥å†…ï¼‰
    """
    if not raw_title or not raw_title.strip():
        return fallback

    # èª¬æ˜æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»ï¼ˆã€Œã‚¿ã‚¤ãƒˆãƒ«:ã€ã€Œã“ã®PDFã¯ã€ç­‰ï¼‰
    cleaned = re.sub(r'^(?:ã‚¿ã‚¤ãƒˆãƒ«[:ï¼š]|ã“ã®PDFã¯|æœ¬è³‡æ–™ã¯|å†…å®¹[:ï¼š])\s*', '', raw_title.strip())

    # å¼•ç”¨ç¬¦ãƒ»æ”¹è¡Œã‚’é™¤å»
    cleaned = cleaned.replace('"', '').replace("'", '').replace('\n', ' ').strip()

    # è¤‡æ•°è¡Œã®å ´åˆã¯æœ€åˆã®è¡Œã®ã¿ä½¿ç”¨
    if '\n' in cleaned:
        cleaned = cleaned.split('\n')[0].strip()

    # 15æ–‡å­—åˆ¶é™
    if len(cleaned) > 15:
        cleaned = cleaned[:15]

    # æœ€çµ‚æ¤œè¨¼: ç©ºã¾ãŸã¯çŸ­ã™ãã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if not cleaned or len(cleaned) < 3:
        return fallback

    return cleaned

# =======================
# Node A: æƒ…å ±åé›†ï¼ˆPDF/YouTube/Tavilyå¯¾å¿œï¼‰
# =======================
@traceable(run_name="a_collect_info")
def collect_info(state: State) -> State:
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  if not topic or not topic.strip():
    topic = "AIæœ€æ–°æƒ…å ±"

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    if input_type == "pdf":
      result = process_pdf(topic)
      data = json.loads(result)

      if data["status"] == "success":
        # PDFã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´å½¢ï¼ˆâ˜…å…¨æ–‡ä¿æŒã«å¤‰æ›´ï¼‰
        pdf_filename = Path(topic).stem
        full_content = data['content']  # ãƒãƒ£ãƒ³ã‚¯åŒºåˆ‡ã‚Š "---" ã§çµåˆæ¸ˆã¿
        context_md = f"# PDF: {pdf_filename}\n\n{full_content}"

        # sourcesã«ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
        chunks = full_content.split("\n\n---\n\n")
        sources = {
          "pdf_content": [{
            "title": pdf_filename,
            "url": topic,
            "content": data['content'][:500],  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨
            "num_pages": data.get('num_pages', 0),
            "total_chars": data.get('total_chars', 0),
            "num_chunks": len(chunks),
            "chunks": chunks  # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿æŒ
          }]
        }

        return {
          "sources": sources,
          "context_md": context_md,
          "log": _log(state, f"[pdf] pages={data.get('num_pages')}, chars={data.get('total_chars')}, chunks={len(chunks)}")
        }
      else:
        return {"error": f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {data['message']}", "log": _log(state, f"[pdf] ERROR {data['message']}")}

    # YouTubeå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
    elif input_type == "youtube":
      return {"error": "YouTubeå‡¦ç†ã¯æº–å‚™ä¸­ã§ã™ï¼ˆIssue #18ã§å®Ÿè£…äºˆå®šï¼‰", "log": _log(state, "[youtube] NOT_IMPLEMENTED")}

    # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ï¼ˆæ—¢å­˜ã®Tavilyæ¤œç´¢ï¼‰
    else:
      # JSTã®ç¾åœ¨æ—¥æ™‚ ã¨ã€€æœˆè‹±èªè¡¨è¨˜ã‚’å–å¾—
      def month_en_for(dt: datetime) -> str:
        months = ["January","February","March","April","May","June",
                  "July","August","September","October","November","December"]
        return f"{months[dt.month-1]} {dt.year}"

      now = now_jst()
      # å…ˆæœˆã¯ã€æœˆæœ«æ—¥ã®1æ—¥ã«ãªã‚‹
      prev_month_dt = now.replace(day=1) - timedelta(days=1)
      # ç›´è¿‘2ãƒ¶æœˆ
      month_labels = [month_en_for(now), month_en_for(prev_month_dt)]

      # ãƒ™ãƒ³ãƒ€ãƒ¼æ¯ã®å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³
      vendors_domains = [
            (["azure.microsoft.com","news.microsoft.com","learn.microsoft.com"],
             ["Microsoft AI updates", "Azure OpenAI updates"]),
            (["openai.com"], ["OpenAI announcements","OpenAI updates"]),
            (["blog.google","ai.googleblog.com","research.google"], ["Google AI updates", "Gemini updates"]),
            (["aws.amazon.com"], ["AWS Bedrock updates","Amazon AI updates"]),
            (["ai.meta.com"], ["Meta AI updates", "Llama updates"]),
            (["anthropic.com"], ["Anthropic Claude updates","Claude announcements"]),
        ]

      # â˜… ç›´è¿‘2ãƒ¶æœˆ Ã— å„ç¤¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹æˆ
      queries: List[Dict[str, Any]] = []
      for m in month_labels:
        for domains, patterns in vendors_domains:
          for p in patterns:
            queries.append({"q": f"{p} {m}", "include_domains": domains, "time_range": "month"})

      sources = tavily_collect_context(queries, max_per_query=6, default_time_range="month")
      context_md = context_to_bullets(sources)
      return {
        "sources": sources,
        "context_md": context_md,
        "log": _log(state, f"[tavily] months={month_labels} queries={len(queries)}")
      }

  except Exception as e:
    return {"error": f"collect_info_error: {e}", "log": _log(state, f"[collect_info] EXCEPTION {e}")}

# -------------------
# Node B: é‡è¦ãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
# -------------------
@traceable(run_name="b_generate_key_points")
def generate_key_points(state: State) -> Dict:
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  if not topic or not topic.strip():
    topic = "AIæœ€æ–°æƒ…å ±"
  ctx = state.get("context_md") or ""
  sources = state.get("sources") or {}

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯Map-Reduceæ–¹å¼ã§å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
  if input_type == "pdf":
    try:
      # ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = ctx.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # Map: å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºï¼ˆæœ€å¤§3å€‹ï¼‰
      # ä¸¦åˆ—å‡¦ç†ã§é«˜é€ŸåŒ–ï¼ˆllm.batchä½¿ç”¨ï¼‰
      valid_chunks = [(i+1, chunk) for i, chunk in enumerate(chunks[:20]) if chunk.strip()]

      if not valid_chunks:
        return {"error": "No valid chunks to process", "log": _log(state, "[key_points_map] no valid chunks")}

      # ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
      batch_prompts = [
        get_key_points_map_prompt(chunk=chunk, chunk_index=idx)
        for idx, chunk in valid_chunks
      ]

      # ä¸¦åˆ—å®Ÿè¡Œï¼ˆæœ€å¤§5ä¸¦åˆ—ï¼‰
      responses = llm.batch(batch_prompts, config={"max_concurrency": 5})

      # çµæœã‚’çµ±åˆ
      chunk_points = []
      for msg in responses:
        points = _strip_bullets(msg.content.splitlines())[:3]
        chunk_points.extend(points)

      # Reduce: å…¨ãƒã‚¤ãƒ³ãƒˆã‚’çµ±åˆã—ã¦5ã¤ã«å‡ç¸®
      if chunk_points:
        reduce_prompt = get_key_points_reduce_prompt(chunk_points=chunk_points)

        msg = llm.invoke(reduce_prompt)
        lines = msg.content.splitlines()

        # å‰ç½®ãè¡Œã‚’é™¤å¤–ï¼ˆç®‡æ¡æ›¸ãè¨˜å·ã¾ãŸã¯ç•ªå·ã§å§‹ã¾ã‚‹è¡Œã®ã¿æŠ½å‡ºï¼‰
        filtered_lines = [
          line for line in lines
          if line.strip() and (
            line.strip().startswith(('-', 'â€¢', '*', 'ãƒ»')) or
            re.match(r'^\d+\.', line.strip())
          )
        ]

        final_bullets = _strip_bullets(filtered_lines)[:5] if filtered_lines else chunk_points[:5]

        # 5å€‹æœªæº€ã®å ´åˆã¯chunk_pointsã‹ã‚‰è£œå……
        while len(final_bullets) < 5 and chunk_points:
          candidate = chunk_points.pop(0)
          if candidate not in final_bullets:  # é‡è¤‡å›é¿
            final_bullets.append(candidate)
      else:
        final_bullets = ["å†…å®¹ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"]

      return {
        "key_points": final_bullets,
        "log": _log(state, f"[key_points_map_reduce] chunks={len(chunks)}, extracted={len(chunk_points)}, final={len(final_bullets)}")
      }

    except Exception as e:
      return {"error": f"key_points_pdf_error: {e}", "log": _log(state, f"[key_points_pdf] EXCEPTION {e}")}

  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = get_key_points_ai_prompt(context_md=ctx, topic=topic)

    try:
      msg = llm.invoke(prompt)
      bullets = _strip_bullets(msg.content.splitlines())[:5] or [msg.content.strip()]
      return {"key_points": bullets, "log": _log(state, f"[key_points] {bullets}")}
    except Exception as e:
      return {"error": f"key_points_error: {e}", "log": _log(state, f"[key_points] EXCEPTION {e}")}

# -------------------
# Node C: ç›®æ¬¡ç”Ÿæˆ
# -------------------
@traceable(run_name="c_generate_toc")
def generate_toc(state: State) -> Dict:
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  if not topic or not topic.strip():
    topic = "AIæœ€æ–°æƒ…å ±"
  key_points = state.get("key_points") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯ä¸­å­¦ç”Ÿå‘ã‘ã®ç« ç«‹ã¦
  if input_type == "pdf":
    prompt = get_toc_pdf_prompt(key_points=key_points)
  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = get_toc_ai_prompt(key_points=key_points)

  try:
    msg = llm.invoke(prompt)
    try:
      data = json.loads(_find_json(msg.content) or msg.content)
      toc = [s.strip() for s in data.get("toc", []) if s.strip()]
    except Exception as e:
      toc = _strip_bullets(msg.content.splitlines())
      toc = toc[:8] or ["ã¯ã˜ã‚ã«", "èƒŒæ™¯", "å®Ÿè£…æ‰‹é †", "è©•ä¾¡ã¨æ”¹å–„", "å…¬é–‹ãƒ»é‹ç”¨", "ã¾ã¨ã‚"]
    return {"toc": toc, "error": "", "log": _log(state, f"[toc] {toc}")}
  except Exception as e:
    return {"error": f"toc_error: {e}", "log": _log(state, f"[toc] EXCEPTION {e}")}

# -------------------
# Mermaidå›³è§£ç”Ÿæˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ï¼ˆIssue #25ï¼‰
# -------------------
# ä»¥ä¸‹ã®é–¢æ•°ã¯å»ƒæ­¢ï¼ˆLLMãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‹¬è‡ªã®å›³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ä¸è¦ï¼‰
# def _generate_architecture_flowchart(key_points: List[str]) -> str:
# def _generate_use_case_mindmap(key_points: List[str]) -> str:


def _insert_after_section(slide_md: str, section_title: str, content: str) -> str:
    """æŒ‡å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ç›´å¾Œã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŒ¿å…¥ï¼ˆh1/h2/h3å¯¾å¿œï¼‰"""
    import re

    # "# section_title" ã¾ãŸã¯ "## section_title" ã®å¾Œã® "---" ã‚’è¦‹ã¤ã‘ã¦æŒ¿å…¥
    # contentã®å…ˆé ­ã¨æœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰ã€åŒºåˆ‡ã‚Šã‚’è¿½åŠ ã—ã¦æŒ¿å…¥
    clean_content = content.strip('\n')
    pattern = rf'(#+\s+{re.escape(section_title)}.*?\n---\s*\n)'

    if re.search(pattern, slide_md, re.DOTALL):
        return re.sub(pattern, rf'\1\n{clean_content}\n\n---\n\n', slide_md, count=1, flags=re.DOTALL)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›®æ¬¡/Agendaã®å¾Œã«æŒ¿å…¥
        agenda_pattern = r'(#+\s+(?:ç›®æ¬¡|Agenda).*?\n---\s*\n)'
        if re.search(agenda_pattern, slide_md, re.DOTALL):
            return re.sub(agenda_pattern, rf'\1\n{clean_content}\n\n---\n\n', slide_md, count=1, flags=re.DOTALL)
        return slide_md


def _insert_before_section(slide_md: str, section_title: str, content: str) -> str:
    """æŒ‡å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³ç›´å‰ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŒ¿å…¥ï¼ˆh1/h2/h3å¯¾å¿œï¼‰"""
    import re

    # contentã®å…ˆé ­ã¨æœ«å°¾ã®æ”¹è¡Œã‚’å‰Šé™¤
    clean_content = content.strip('\n')

    # ãƒ‘ã‚¿ãƒ¼ãƒ³: --- ã®å¾Œã« section_title ãŒã‚ã‚‹ç®‡æ‰€
    # ãƒãƒƒãƒã‚°ãƒ«ãƒ¼ãƒ—1: --- + æ”¹è¡Œã€ã‚°ãƒ«ãƒ¼ãƒ—2: section_title
    pattern = rf'(---\s*\n\n)(#+\s+{re.escape(section_title)})'

    if re.search(pattern, slide_md):
        # --- ã¨ section_title ã®é–“ã«å›³è§£ã‚’æŒ¿å…¥
        return re.sub(pattern, rf'\1{clean_content}\n\n---\n\n\2', slide_md, count=1)
    else:
        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœ«å°¾ã«è¿½åŠ 
        return slide_md.rstrip('\n') + f'\n\n{clean_content}\n\n---\n\n'

# -------------------
# Node D: ã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡ï¼ˆSlidevï¼‰ç”Ÿæˆ
# -------------------
@traceable(run_name="d_generate_slide_slidev")
def write_slides_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆï¼ˆå…¨6ç¤¾å¯¾å¿œ / PDFå¯¾å¿œï¼‰"""
  sources = state.get("sources") or {}
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  if not topic or not topic.strip():
    topic = "AIæœ€æ–°æƒ…å ±"
  context_md = state.get("context_md") or ""
  key_points = state.get("key_points") or []
  toc = state.get("toc") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ã®å ´åˆã¯æ±ç”¨çš„ãªã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    if input_type == "pdf":
      # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¦ç´„ã‚’ä½œæˆã—ã¦ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = context_md.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # PDFãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’æº–å‚™
      if topic.endswith('.pdf') or '/uploads/' in topic:
          pdf_filename = Path(topic).name.replace('.pdf', '')
          # UUIDéƒ¨åˆ†ã‚’é™¤å»ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³: uuid_filenameï¼‰
          if '_' in pdf_filename:
              parts = pdf_filename.split('_', 1)
              if len(parts) > 1 and len(parts[0]) > 30:  # UUIDã£ã½ã„é•·ã„æ–‡å­—åˆ—
                  pdf_filename = parts[1]
          fallback_title = pdf_filename[:15] if pdf_filename else "PDFè³‡æ–™"
      else:
          fallback_title = "è³‡æ–™"

      # PDFã®å†…å®¹ã‹ã‚‰LLMã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
      title_prompt = get_slide_title_prompt(chunks=chunks, key_points=key_points)

      try:
        title_msg = llm.invoke(title_prompt)
        raw_title = title_msg.content.strip()
        ja_title = extract_clean_title(raw_title, fallback=fallback_title)
      except Exception as e:
        ja_title = fallback_title
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¿½åŠ 
        print(f"[slide_workflow] ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼ (fallbackä½¿ç”¨): {str(e)[:100]}")

      # ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆè¦ç´„ãªã—ãƒ»ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‰ˆï¼‰
      chunk_texts = []
      total_chars = 0
      for i, chunk in enumerate(chunks[:20]):
        if not chunk.strip():
          continue

        # å„ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­1500æ–‡å­—ã‚’ä½¿ç”¨
        chunk_preview = chunk[:1500]
        chunk_texts.append(f"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i+1}\n{chunk_preview}")
        total_chars += len(chunk_preview)

        # åˆè¨ˆ15,000æ–‡å­—ã¾ã§ä½¿ç”¨ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã®ä¸Šé™ï¼‰
        if total_chars > 15000:
          break

      # å…¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
      full_summary = "\n\n".join(chunk_texts)

      # LLMã§Slidevãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯æŠœç²‹ç‰ˆã‚’ä½¿ç”¨ï¼‰
      prompt = get_slide_pdf_prompt(
        full_summary=full_summary,
        key_points=key_points,
        toc=toc,
        ja_title=ja_title
      )

      msg = llm.invoke(prompt)
      raw_content = msg.content.strip()

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # æ§‹é€ åˆ¶å¾¡ï¼ˆPythonå´ã§æ©Ÿæ¢°çš„ã«å®Ÿæ–½ï¼‰
      # è¨­è¨ˆæ–¹é‡: docs/architecture/SLIDE_GENERATION_DESIGN.md
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # Step 1: ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯é™¤å»ï¼ˆæ—¢å­˜ï¼‰
      raw_content = _strip_whole_code_fence(raw_content)

      # Step 2: æ—¢å­˜ã®ãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼å‰Šé™¤ï¼ˆLLMãŒå‹æ‰‹ã«ç”Ÿæˆã—ãŸå ´åˆã®ä¿é™ºï¼‰
      raw_content = re.sub(r'^---[\s\S]*?---\s*', '', raw_content.strip(), count=1)

      # Step 3: YAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ç”Ÿæˆï¼ˆPythonå´ã§åˆ¶å¾¡ï¼‰
      frontmatter = """---
theme: apple-basic
highlighter: shiki
class: text-center
---

"""

      # Step 4: è¦‹å‡ºã—ï¼ˆ## ï¼‰å‰ã« `---` ã‚’æ©Ÿæ¢°çš„ã«æŒ¿å…¥
      content_with_separators = _insert_separators(raw_content)

      # Step 4.5: ä¼šè©±å½¢å¼ã«æ”¹è¡Œã‚’æŒ¿å…¥ï¼ˆLLMå‡ºåŠ›ã®æºã‚‰ãã«å¯¾å¿œï¼‰
      content_with_separators = _format_conversation(content_with_separators)

      # Step 5: é€£ç¶šã—ãŸ --- ã‚’åœ§ç¸®ï¼ˆå®‰å…¨è£…ç½®ï¼‰
      content_with_separators = _double_separators(content_with_separators)

      # Step 6: æœ€çµ‚çš„ãªMarkdownç”Ÿæˆ
      slide_md = frontmatter + content_with_separators

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev_pdf] generated ({len(slide_md)} chars) from {len(chunk_texts)} chunks with mechanical structure control")
      }

    # AIæœ€æ–°æƒ…å ±ï¼ˆTavilyï¼‰ã®å ´åˆã¯æ—¢å­˜ã®ãƒãƒ«ãƒãƒ™ãƒ³ãƒ€ãƒ¼ç”Ÿæˆ
    else:
      ja_title = f"{month_ja()} AIæœ€æ–°æƒ…å ±ã¾ã¨ã‚"
      slide_md = _generate_multi_vendor_slides_integrated(
        topic=ja_title,
        sources=sources,
        mvp_version="AI Industry Report 2025"
      )

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev] generated ({len(slide_md)} chars, 6 vendors)")
      }

  except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    fallback_md = f"""---
theme: apple-basic
highlighter: shiki
class: text-center
---

# ğŸš€ {ja_title}
## ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ

<div class="pt-12">
  <span class="px-2 py-1 rounded" style="background: #ef4444; color: white;">
    Error: {str(e)}
  </span>
</div>

---

## âš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°

ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

- æ¤œç´¢çµæœã®å–å¾—ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„

"""
    return {
      "slide_md": fallback_md,
      "title": ja_title,
      "error": f"slides_slidev_error: {e}",
      "log": _log(state, f"[slides_slidev] EXCEPTION {e} - using fallback")
    }

# -------------------
# Node D.5: Mermaidå›³è§£ç”Ÿæˆï¼ˆIssue #25ï¼‰
# -------------------
@traceable(run_name="d5_generate_diagrams")
# generate_diagrams ãƒãƒ¼ãƒ‰ã¯å»ƒæ­¢ï¼ˆLLMãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç‹¬è‡ªã®å›³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ä¸è¦ï¼‰
# Issue #25: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå›³ã®å¼·åˆ¶æŒ¿å…¥ã‚’å‰Šé™¤ã—ã€LLMã«ã‚ˆã‚‹ç‹¬è‡ªå›³ç”Ÿæˆã«ç§»è¡Œ
def generate_diagrams(state: State) -> Dict:
    """[DEPRECATED] ã“ã®ãƒãƒ¼ãƒ‰ã¯ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“"""
    return {"log": _log(state, "[diagrams] deprecated - skipped")}

# -------------------
# Node E: è©•ä¾¡
# -------------------
MAX_ATTEMPTS = 3

# Slidevç”¨è©•ä¾¡ãƒãƒ¼ãƒ‰
@traceable(run_name="e_evaluate_slides_slidev")
def evaluate_slides_slidev(state: State) -> Dict:
  """Slidevã‚¹ãƒ©ã‚¤ãƒ‰ã®å“è³ªè©•ä¾¡ï¼ˆPDF/AIæƒ…å ±å¯¾å¿œï¼‰"""
  if state.get("error"):
    return {}
  slide_md = state.get("slide_md") or ""
  toc = state.get("toc") or []
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  if not topic or not topic.strip():
    topic = "AIæœ€æ–°æƒ…å ±"

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã—ã¦PDFç‰¹æœ‰ã®è©•ä¾¡åŸºæº–ã‚’è¿½åŠ 
  input_type = detect_input_type(topic)

  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆå…¥åŠ›ã‚¿ã‚¤ãƒ—ã§è©•ä¾¡åŸºæº–ã‚’åˆ‡ã‚Šæ›¿ãˆï¼‰
  prompt = get_evaluation_prompt(
    slide_md=slide_md,
    toc=toc,
    topic=topic,
    input_type=input_type
  )
  try:
    msg = llm.invoke(prompt)
    raw = msg.content or ""
    js = _find_json(raw) or raw
    data = json.loads(js)

    score = float(data.get("score", 0.0))
    subscores = data.get("subscores") or {}
    reasons = data.get("reasons") or {}
    suggestions = data.get("suggestions") or []
    risk_flags = data.get("risk_flags") or []
    passed = bool(data.get("pass", score >= 8.0))
    feedback = str(data.get("feedback", "")).strip()
    attempts = (state.get("attempts") or 0) + 1

    return {
      "score": score,
      "subscores": subscores,
      "reasons": reasons,
      "suggestions": suggestions,
      "risk_flags": risk_flags,
      "passed": passed,
      "feedback": feedback,
      "attempts": attempts,
      "log": _log(state, f"[evaluate_slidev] score={score:.2f} pass={passed} attempts={attempts}")
    }
  except Exception as e:
    return {"error": f"eval_error: {e}", "log": _log(state, f"[evaluate_slidev] EXCEPTION {e}")}

def route_after_eval_slidev(state: State) -> str:
    """è©•ä¾¡çµæœã«åŸºã¥ã„ã¦ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯å®Œäº†ã‚’åˆ¤å®š"""
    if (state.get("attempts") or 0) >= MAX_ATTEMPTS:
        return "ok"
    return "ok" if state.get("passed") else "retry"

# -------------------
# Node F: ä¿å­˜ & Slidevãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
# -------------------
@traceable(run_name="f_save_and_render_slidev")
def save_and_render_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä¿å­˜ï¼ˆPDFç”Ÿæˆå»ƒæ­¢ã€MDä¿å­˜ã®ã¿ï¼‰"""
  if state.get("error"):
    return {}

  slide_md = state.get("slide_md") or ""
  title = state.get("title") or "AIã‚¹ãƒ©ã‚¤ãƒ‰"

  # ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
  if not slide_md.strip():
    return {
      "error": "slide_md is empty",
      "log": _log(state, "[save_slidev] ERROR: slide_md is empty")
    }

  # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«åã®è‹±èªè¡¨è¨˜ã‚’ç”Ÿæˆ
  slug_prompt = get_slug_prompt(title=title)

  try:
    emsg = llm.invoke(slug_prompt)
    file_stem = _slugify_en(emsg.content.strip()) or _slugify_en(title)
  except Exception:
    file_stem = _slugify_en(title) or "ai-latest-info"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Supabase Storage & DBä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã€å¤±æ•—ã—ã¦ã‚‚ç¶™ç¶šï¼‰
  # PDFç”Ÿæˆå»ƒæ­¢: MDä¿å­˜ã®ã¿ï¼ˆãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å‡¦ç†å›é¿ï¼‰
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  user_id = state.get("user_id", "anonymous")
  topic = state.get("topic", "AIæœ€æ–°æƒ…å ±")
  md_url = None
  log_msg = ""

  try:
    # Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    md_storage_path = f"{user_id}/{file_stem}_slidev.md"
    md_url = upload_to_storage(
      bucket="slide-files",
      file_path=md_storage_path,
      file_data=slide_md.encode("utf-8"),
      content_type="text/markdown"
    )
    log_msg = f"[save_slidev] MD uploaded to {md_url}"

  except Exception as e:
    log_msg = f"[save_slidev] Storage upload failed: {str(e)}"

  # Supabase DBã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
  result = {
    "slide_path": md_url,
    "md_url": md_url,
    "log": _log(state, log_msg)
  }

  try:
    supabase_result = save_slide_to_supabase(
      user_id=user_id,
      title=title,
      topic=topic,
      slide_md=slide_md,
      pdf_url=None  # PDFå»ƒæ­¢
    )

    if "slide_id" in supabase_result:
      result["slide_id"] = supabase_result["slide_id"]
      result["log"] = _log(state, f"[supabase] saved slide_id={supabase_result['slide_id']}")
    elif "error" in supabase_result:
      result["log"] = _log(state, f"[supabase] {supabase_result['error']}")

  except Exception as e:
    result["log"] = _log(state, f"[supabase] save failed (non-critical): {str(e)[:100]}")

  return result

# -------------------
# Node G: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆOpenAI TTSï¼‰
# -------------------
def _parse_slide_to_json(slide_content: str, index: int) -> Dict[str, Any]:
    """
    ã‚¹ãƒ©ã‚¤ãƒ‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’æ§‹é€ åŒ–JSONã«å¤‰æ›

    Args:
        slide_content: ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å†…å®¹
        index: ã‚¹ãƒ©ã‚¤ãƒ‰ç•ªå·ï¼ˆ0å§‹ã¾ã‚Šï¼‰

    Returns:
        ã‚¹ãƒ©ã‚¤ãƒ‰ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
    """
    import re

    lines = [l for l in slide_content.strip().split("\n") if l.strip()]
    if not lines:
        return {"type": "content", "heading": f"ã‚¹ãƒ©ã‚¤ãƒ‰ {index + 1}", "bullets": []}

    # ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰åˆ¤å®šï¼ˆ# ã§å§‹ã¾ã‚Šã€ç®‡æ¡æ›¸ããŒãªã„ï¼‰
    if lines[0].startswith("# ") and not any(l.strip().startswith("-") for l in lines):
        title = lines[0].lstrip("# ").strip()
        subtitle = ""
        for l in lines[1:]:
            if l.strip() and not l.startswith("#"):
                subtitle = l.strip()
                break
        return {"type": "title", "title": title, "subtitle": subtitle}

    content_str = "\n".join(lines)

    # mermaidå›³åˆ¤å®šï¼ˆ```mermaid ãŒå«ã¾ã‚Œã‚‹ï¼‰
    if "```mermaid" in content_str:
        heading = ""
        mermaid_code = ""

        # mermaidã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        mermaid_match = re.search(r'```mermaid\n(.*?)\n```', content_str, re.DOTALL)
        if mermaid_match:
            mermaid_code = mermaid_match.group(1).strip()

        # è¦‹å‡ºã—ã‚’æŠ½å‡º
        for l in lines:
            if l.startswith("## "):
                heading = l.lstrip("## ").strip()
                break
            elif l.startswith("# "):
                heading = l.lstrip("# ").strip()
                break

        if mermaid_code:
            return {
                "type": "mermaid",
                "heading": heading or "å›³è§£",
                "mermaid_code": mermaid_code
            }

    # ä¼šè©±å½¢å¼åˆ¤å®šï¼ˆğŸ‘¨â€ğŸ« ã¾ãŸã¯ å…ˆç”Ÿ: ãŒå«ã¾ã‚Œã‚‹ï¼‰
    if "ğŸ‘¨â€ğŸ«" in content_str or "å…ˆç”Ÿ:" in content_str or "ğŸ§‘â€ğŸ“" in content_str:
        heading = ""
        teacher = ""
        student = ""

        for l in lines:
            if l.startswith("## "):
                heading = l.lstrip("## ").strip()
            elif "ğŸ‘¨â€ğŸ«" in l or "å…ˆç”Ÿ:" in l:
                teacher = re.sub(r"^[-*]\s*", "", l)
                teacher = re.sub(r"(ğŸ‘¨â€ğŸ«|å…ˆç”Ÿ:?)\s*", "", teacher).strip()
            elif "ğŸ§‘â€ğŸ“" in l or "ç”Ÿå¾’:" in l:
                student = re.sub(r"^[-*]\s*", "", l)
                student = re.sub(r"(ğŸ§‘â€ğŸ“|ç”Ÿå¾’:?)\s*", "", student).strip()

        if teacher or student:
            return {
                "type": "conversation",
                "heading": heading or "ä¼šè©±",
                "teacher": teacher,
                "student": student
            }

    # ã¾ã¨ã‚ã‚¹ãƒ©ã‚¤ãƒ‰åˆ¤å®š
    if any(keyword in content_str for keyword in ["ã¾ã¨ã‚", "ãƒã‚¤ãƒ³ãƒˆ", "è¦ç‚¹", "Summary"]):
        heading = ""
        points = []
        for l in lines:
            if l.startswith("## "):
                heading = l.lstrip("## ").strip()
            elif l.strip().startswith("-") or l.strip().startswith("*"):
                point = re.sub(r"^[-*]\s*", "", l.strip())
                if point:
                    points.append(point)

        if points:
            return {
                "type": "summary",
                "heading": heading or "ã¾ã¨ã‚",
                "points": points
            }

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¹ãƒ©ã‚¤ãƒ‰ï¼ˆè¦‹å‡ºã— + ç®‡æ¡æ›¸ãï¼‰
    heading = ""
    bullets = []
    for l in lines:
        if l.startswith("## "):
            heading = l.lstrip("## ").strip()
        elif l.startswith("# "):
            heading = l.lstrip("# ").strip()
        elif l.strip().startswith("-") or l.strip().startswith("*"):
            bullet = re.sub(r"^[-*]\s*", "", l.strip())
            if bullet:
                bullets.append(bullet)
        elif l.strip() and not heading:
            # è¦‹å‡ºã—ãŒã¾ã ãªã„å ´åˆã€æœ€åˆã®éç©ºè¡Œã‚’è¦‹å‡ºã—ã«
            heading = l.strip()

    return {
        "type": "content",
        "heading": heading or f"ã‚¹ãƒ©ã‚¤ãƒ‰ {index + 1}",
        "bullets": bullets
    }


@traceable(run_name="g_generate_narration")
def generate_narration(state: State) -> Dict:
    """å„ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³éŸ³å£°ã‚’ç”Ÿæˆï¼ˆOpenAI TTSï¼‰+ slides_jsonç”Ÿæˆ

    ä¸¦åˆ—å‡¦ç†:
    - LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ: llm.batch() ã§æœ€å¤§5ä¸¦åˆ—
    - TTSéŸ³å£°ç”Ÿæˆ: ThreadPoolExecutor ã§æœ€å¤§5ä¸¦åˆ—
    """
    from openai import OpenAI
    from concurrent.futures import ThreadPoolExecutor
    from app.prompts.narration_prompts import get_narration_prompt

    if state.get("error"):
        return {}

    slide_md = state.get("slide_md", "")
    title = state.get("title", "AIã‚¹ãƒ©ã‚¤ãƒ‰")

    # Slidevã®ã‚¹ãƒ©ã‚¤ãƒ‰åŒºåˆ‡ã‚Šï¼ˆ---ï¼‰ã§åˆ†å‰²
    slides = slide_md.split("\n---\n")

    # frontmatterï¼ˆæœ€åˆã®YAMLéƒ¨åˆ†ï¼‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
    slide_contents = []
    slides_json = []

    for idx, slide in enumerate(slides[1:]):  # slides[0]ã¯frontmatter
        # ç©ºç™½ãƒ»ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’é™¤å»
        content = "\n".join([
            line for line in slide.split("\n")
            if line.strip() and not line.strip().startswith("<!--")
        ])
        if content.strip():
            slide_contents.append(content)
            # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’JSONå½¢å¼ã«å¤‰æ›
            slide_data = _parse_slide_to_json(content, idx)
            slides_json.append(slide_data)

    if not slide_contents:
        return {
            "error": "No slide content found for narration",
            "log": _log(state, "[narration] ERROR: no valid slides")
        }

    # OpenAI TTSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = OpenAI()  # OPENAI_API_KEYã‹ã‚‰è‡ªå‹•èªè¨¼

    # è¨­å®šå€¤å–å¾—
    tts_model = getattr(settings, 'TTS_MODEL', 'tts-1-hd')
    tts_voice = getattr(settings, 'TTS_VOICE', 'shimmer')
    tts_speed = float(getattr(settings, 'TTS_SPEED', '1.0'))

    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    temp_dir = Path(tempfile.mkdtemp())

    try:
        # ========== Step 1: LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ ==========
        batch_prompts = [
            get_narration_prompt(slide_content=content)
            for content in slide_contents
        ]

        # ä¸¦åˆ—LLMå®Ÿè¡Œï¼ˆæœ€å¤§5ä¸¦åˆ—ï¼‰
        responses = llm.batch(batch_prompts, config={"max_concurrency": 5})

        # çµæœã‚’æ•´å½¢
        narrations = []
        for i, msg in enumerate(responses):
            try:
                narration_text = msg.content.strip().strip('"').strip("'")
                narrations.append(narration_text)
            except Exception as e:
                # LLMã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                narrations.append(f"{i+1}æšç›®ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚")
                print(f"[narration] LLM parse error for slide {i}: {str(e)[:100]}")

        # ========== Step 2: TTSéŸ³å£°ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰ ==========
        def generate_audio(args):
            """å˜ä¸€ã‚¹ãƒ©ã‚¤ãƒ‰ã®éŸ³å£°ç”Ÿæˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ç”¨ï¼‰"""
            idx, narration_text = args
            response = client.audio.speech.create(
                model=tts_model,
                voice=tts_voice,
                input=narration_text,
                speed=tts_speed
            )
            audio_path = temp_dir / f"narration_{idx:03d}.mp3"
            with open(audio_path, 'wb') as f:
                f.write(response.content)
            return idx, str(audio_path)

        # ä¸¦åˆ—TTSå®Ÿè¡Œï¼ˆæœ€å¤§5ä¸¦åˆ—ï¼‰
        audio_results = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [
                executor.submit(generate_audio, (i, narration))
                for i, narration in enumerate(narrations)
            ]
            for future in futures:
                try:
                    result = future.result()
                    audio_results.append(result)
                except Exception as e:
                    # TTSå¤±æ•—æ™‚ã¯å³åº§ã«ã‚¨ãƒ©ãƒ¼è¿”å´
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    return {
                        "error": f"OpenAI TTS error: {str(e)}",
                        "log": _log(state, f"[narration] TTS API failed: {str(e)[:100]}")
                    }

        # é †åºã‚’ç¶­æŒã—ã¦ã‚½ãƒ¼ãƒˆï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ï¼‰
        audio_results.sort(key=lambda x: x[0])
        audio_files = [path for _, path in audio_results]

        return {
            "narration_scripts": narrations,
            "audio_files": audio_files,
            "slides_json": slides_json,  # HTMLç”Ÿæˆç”¨ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
            "_temp_narration_dir": str(temp_dir),  # å¾Œç¶šãƒãƒ¼ãƒ‰ã§ä½¿ç”¨
            "log": _log(state, f"[narration] generated {len(audio_files)} audio files, {len(slides_json)} slides_json (model={tts_model}, voice={tts_voice}, parallel=5)")
        }

    except Exception as e:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        shutil.rmtree(temp_dir, ignore_errors=True)

        return {
            "error": f"narration_error: {str(e)}",
            "log": _log(state, f"[narration] EXCEPTION {str(e)[:100]}")
        }

# -------------------
# Node H: å‹•ç”»ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ï¼ˆCloud Run Job éåŒæœŸç‰ˆï¼‰
# Cloud Run Jobã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã€job_idã‚’è¿”ã™ã€‚å®Ÿéš›ã®å‹•ç”»ç”Ÿæˆã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã€‚
# -------------------
@traceable(run_name="h_render_video")
def render_video(state: State) -> Dict:
    """PNGç”»åƒ + éŸ³å£° â†’ MP4å‹•ç”»ç”Ÿæˆï¼ˆCloud Run Job éåŒæœŸç‰ˆï¼‰

    Cloud Run Jobã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã€video_job_idã‚’è¿”ã™ã€‚
    å®Ÿéš›ã®å‹•ç”»ç”Ÿæˆã¯ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ãªã„ã€‚
    ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ /api/video/status/{job_id} ã§ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚

    é‡è¦: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‹ã‚‰æ¸¡ã™ã€‚
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¯éåŒæœŸã‚¸ãƒ§ãƒ–å®Ÿè¡Œæ™‚ã«å­˜åœ¨ã—ãªã„ãŸã‚ã€‚
    """
    import httpx
    import os
    from pathlib import Path
    from app.core.storage import upload_to_storage

    print("[DEBUG] render_video: START (Cloud Run Job async)")

    if state.get("error"):
        print("[DEBUG] render_video: error in state, returning early")
        return {}

    audio_files = state.get("audio_files", [])
    slides_json = state.get("slides_json", [])
    temp_narration_dir = state.get("_temp_narration_dir")
    title = state.get("title", "AIã‚¹ãƒ©ã‚¤ãƒ‰")
    user_id = state.get("user_id", "anonymous")
    slide_id = state.get("slide_id", "")

    print(f"[DEBUG] render_video: audio_files={len(audio_files)}, slides_json={len(slides_json)}")

    if not audio_files:
        print("[DEBUG] render_video: no audio files")
        return {
            "error": "No audio files for video rendering",
            "log": _log(state, "[video] ERROR: no audio files")
        }

    if not slides_json:
        print("[DEBUG] render_video: no slides_json")
        return {
            "error": "No slides_json for video rendering",
            "log": _log(state, "[video] ERROR: no slides_json data")
        }

    if not slide_id:
        print("[DEBUG] render_video: no slide_id (required for async)")
        return {
            "error": "No slide_id for async video rendering",
            "log": _log(state, "[video] ERROR: slide_id is required for async rendering")
        }

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    # éåŒæœŸã‚¸ãƒ§ãƒ–ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãŸã‚
    print("[DEBUG] render_video: uploading audio files to Supabase Storage")
    audio_urls = []
    try:
        for i, audio_path in enumerate(audio_files):
            audio_file = Path(audio_path)
            if not audio_file.exists():
                print(f"[DEBUG] render_video: audio file not found: {audio_path}")
                return {
                    "error": f"Audio file not found: {audio_path}",
                    "log": _log(state, f"[video] ERROR: audio file not found: {audio_path}")
                }

            # Supabase Storageã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            storage_path = f"{user_id}/narration/{slide_id}/narration_{i:03d}.mp3"
            audio_url = upload_to_storage(
                bucket="slide-files",
                file_path=storage_path,
                file_data=audio_file.read_bytes(),
                content_type="audio/mpeg"
            )
            audio_urls.append(audio_url)
            print(f"[DEBUG] render_video: uploaded audio {i} -> {audio_url}")

        print(f"[DEBUG] render_video: uploaded {len(audio_urls)} audio files")
    except Exception as e:
        print(f"[DEBUG] render_video: audio upload failed: {e}")
        return {
            "error": f"Audio upload failed: {str(e)}",
            "log": _log(state, f"[video] ERROR: audio upload failed: {str(e)[:100]}")
        }

    # FastAPIçµŒç”±ã§éåŒæœŸå‹•ç”»ç”Ÿæˆã‚¸ãƒ§ãƒ–ã‚’ãƒˆãƒªã‚¬ãƒ¼
    fastapi_url = os.getenv("FASTAPI_URL", "http://localhost:8001")
    print(f"[DEBUG] render_video: calling FastAPI at {fastapi_url}/api/render/video/async")

    try:
        with httpx.Client(timeout=30) as client:  # ã‚¸ãƒ§ãƒ–ä½œæˆã¯30ç§’ã§ååˆ†
            response = client.post(
                f"{fastapi_url}/api/render/video/async",
                json={
                    "slides_json": slides_json,
                    "audio_files": audio_urls,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã§ã¯ãªãSupabase URLã‚’æ¸¡ã™
                    "title": title,
                    "user_id": user_id,
                    "slide_id": slide_id
                }
            )
            response.raise_for_status()
            result = response.json()

        job_id = result.get("job_id", "")
        print(f"[DEBUG] render_video: async job created, job_id={job_id}")

        # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if temp_narration_dir:
            shutil.rmtree(temp_narration_dir, ignore_errors=True)

        return {
            "video_job_id": job_id,
            "log": _log(state, f"[video] async job created: {job_id}")
        }

    except httpx.TimeoutException:
        print("[DEBUG] render_video: TIMEOUT creating job")
        return {
            "error": "Video job creation timeout",
            "log": _log(state, "[video] TIMEOUT creating async job")
        }
    except httpx.HTTPStatusError as e:
        print(f"[DEBUG] render_video: HTTP error {e.response.status_code}")
        return {
            "error": f"video_job_error: HTTP {e.response.status_code}",
            "log": _log(state, f"[video] FastAPI error: {e.response.text[:100]}")
        }
    except Exception as e:
        print(f"[DEBUG] render_video: ERROR {e}")
        return {
            "error": f"video_job_error: {str(e)}",
            "log": _log(state, f"[video] ERROR: {str(e)[:100]}")
        }


# -------------------
# æ¡ä»¶åˆ†å²: å‹•ç”»ç”Ÿæˆ
# -------------------
def route_after_save(state: State) -> str:
    """ä¿å­˜å¾Œã®åˆ†å²: å¸¸ã«å‹•ç”»ç”Ÿæˆã¸"""
    # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if state.get("error"):
        return END

    # å¸¸ã«å‹•ç”»ç”Ÿæˆã¸
    return "generate_narration"

# -------------------
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# -------------------
graph_builder = StateGraph(State)
graph_builder.add_node("collect_info", collect_info)
graph_builder.add_node("generate_key_points", generate_key_points)
graph_builder.add_node("generate_toc", generate_toc)
graph_builder.add_node("write_slides_slidev", write_slides_slidev)
graph_builder.add_node("generate_diagrams", generate_diagrams)
graph_builder.add_node("save_and_render_slidev", save_and_render_slidev)
graph_builder.add_node("evaluate_slides_slidev", evaluate_slides_slidev)
graph_builder.add_node("generate_narration", generate_narration)
graph_builder.add_node("render_video", render_video)

# ã‚¨ãƒƒã‚¸å®šç¾©ï¼ˆSlidevãƒ•ãƒ­ãƒ¼ with è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼‰
graph_builder.add_edge(START, "collect_info")
graph_builder.add_edge("collect_info", "generate_key_points")
graph_builder.add_edge("generate_key_points", "generate_toc")
graph_builder.add_edge("generate_toc", "write_slides_slidev")
graph_builder.add_edge("write_slides_slidev", "evaluate_slides_slidev")

# è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
graph_builder.add_conditional_edges(
  "evaluate_slides_slidev",
  route_after_eval_slidev,
  {"retry": "generate_key_points", "ok": "save_and_render_slidev"}
)

# å‹•ç”»ç”Ÿæˆãƒ•ãƒ­ãƒ¼ï¼ˆæ¡ä»¶åˆ†å²ï¼‰
graph_builder.add_conditional_edges(
    "save_and_render_slidev",
    route_after_save,
    {"generate_narration": "generate_narration", END: END}
)

# å‹•ç”»ç”Ÿæˆã‚¨ãƒƒã‚¸
graph_builder.add_edge("generate_narration", "render_video")
graph_builder.add_edge("render_video", END)

graph = graph_builder.compile()

# -------------------
# å®Ÿè¡Œ
# -------------------
if __name__ == "__main__":
  print("LangSmith Tracing:", os.getenv("LANGCHAIN_TRACING_V2"),
      "| Project:", os.getenv("LANGCHAIN_PROJECT"))
  init: State = {
    "topic": "LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavilyã§ä½œã‚‹ï¼šæœ€æ–°AIå‹•å‘ã‚¹ãƒ©ã‚¤ãƒ‰",
    "key_points": [], "toc": [], "slide_md": "",
    "score": 0.0,
    "subscores": {}, "reasons": {},
    "suggestions": [], "risk_flags": [],
    "passed": False, "feedback": "",
    "title": "", "slide_path": "",
    "attempts": 0, "error": "", "log": [],
    "context_md": "", "sources": {}
  }

  config: RunnableConfig = {
    "run_name": "tavily_marp_agent",
    "tags": ["marp", "langgraph", "langsmith", "openai", "tavily"],
    "metadata": {"env": "dev", "date": datetime.now(timezone.utc).isoformat()},
    "recursive_limit": 60,
  }
  out = graph.invoke(init, config=config)

  print("\n=== RESULT ===")
  if out.get("error"):
    print("ERROR:", out["error"])
  else:
    print("Title    :", out.get("title"))
    print("Slide    :", out.get("slide_path"))
    # print("Score    :", out.get("score"))
    # print("Passed   :", out.get("passed"))
  print("\n=== LOGS ===")
  for line in out.get("log", []):
    print(line)