# LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavily Ã— Slidev
# ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ï¼ˆPDF/YouTube/ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œï¼‰
# ãƒ•ãƒ­ãƒ¼: æƒ…å ±åé›† -> ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæŠ½å‡º -> ç›®æ¬¡ç”Ÿæˆ -> ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ -> è©•ä¾¡ -> ä¿å­˜

# å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.core.config import TAVILY_API_KEY, SLIDE_FORMAT, MARP_THEME, MARP_PAGINATE
from src.core.llm import llm
from src.core.utils import (
    # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
    _log, _strip_bullets, _slugify_en, _find_json,
    _ensure_marp_header, _insert_separators, _double_separators,
    _clean_title, _strip_whole_code_fence, _remove_presenter_lines,
    # æ—¥æ™‚é–¢æ•°
    JST, now_jst, today_iso, month_ja, month_en, get_today_jst,
    # Slidevç”Ÿæˆé–¢æ•°
    _get_all_vendors_info, _create_llm_summarized_bullets,
    _generate_multi_vendor_slides_integrated,
    # Tavilyæ¤œç´¢é–¢æ•°
    tavily_search, tavily_collect_context, context_to_bullets,
    # ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
    generate_slidev_test,
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å›ºæœ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from typing_extensions import TypedDict
from typing import Optional, Dict, Any, Union, List
from langsmith import traceable
from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from src.tools.pdf import process_pdf
import os
import re
import json
import shutil
import subprocess
from datetime import datetime, timedelta, timezone
from pathlib import Path


# -------------------
# State
# -------------------
class State(TypedDict):
  """LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®çŠ¶æ…‹ç®¡ç†"""

  # å…¥åŠ›
  topic: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã®ä¸»é¡Œ

  # æƒ…å ±åé›† (Node A)
  sources: Dict[str, List[Dict[str, str]]]      # Tavilyæ¤œç´¢çµæœ
  context_md: str                               # æ¤œç´¢çµæœã®Markdown

  # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ (Node B-D)
  key_points: List[str]                         # é‡è¦ãƒã‚¤ãƒ³ãƒˆ5å€‹
  toc: List[str]                                # ç›®æ¬¡5-8é …ç›®
  slide_md: str                                 # Marpã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡
  title: str                                    # ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«

  # è©•ä¾¡ (Node E)
  score: float                                  # ç·åˆã‚¹ã‚³ã‚¢ (0-10)
  subscores: Dict[str, float]                   # é …ç›®åˆ¥ã‚¹ã‚³ã‚¢
  reasons: Dict[str, str]                       # è©•ä¾¡ç†ç”±
  suggestions: List[str]                        # æ”¹å–„ææ¡ˆ
  risk_flags: List[str]                         # ãƒªã‚¹ã‚¯äº‹é …
  passed: bool                                  # åˆæ ¼åˆ¤å®š (>=8.0)
  feedback: str                                 # ç·åˆãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
  attempts: int                                 # ãƒªãƒˆãƒ©ã‚¤å›æ•° (æœ€å¤§3)

  # å‡ºåŠ› (Node F)
  slide_path: str                               # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

  # ã‚·ã‚¹ãƒ†ãƒ 
  error: str                                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
  log: List[str]                                # å®Ÿè¡Œãƒ­ã‚°

# =======================
# å…¥åŠ›ã‚¿ã‚¤ãƒ—è‡ªå‹•åˆ¤åˆ¥
# =======================
def detect_input_type(topic: str) -> str:
    """
    å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
    Returns: "pdf" | "youtube" | "text"
    """
    topic_lower = topic.lower()

    # YouTube URLãƒ‘ã‚¿ãƒ¼ãƒ³
    youtube_patterns = [
        r'youtube\.com/watch\?v=',
        r'youtu\.be/',
        r'youtube\.com/embed/',
    ]
    for pattern in youtube_patterns:
        if re.search(pattern, topic_lower):
            return "youtube"

    # PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³
    if topic_lower.endswith('.pdf') or '/uploads/' in topic_lower:
        return "pdf"

    # ãã‚Œä»¥å¤–ã¯ãƒ†ã‚­ã‚¹ãƒˆï¼ˆTavilyæ¤œç´¢ï¼‰
    return "text"

# =======================
# Node A: æƒ…å ±åé›†ï¼ˆPDF/YouTube/Tavilyå¯¾å¿œï¼‰
# =======================
@traceable(run_name="a_collect_info")
def collect_info(state: State) -> State:
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’è‡ªå‹•åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    if input_type == "pdf":
      result = process_pdf(topic)
      data = json.loads(result)

      if data["status"] == "success":
        # PDFã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´å½¢ï¼ˆâ˜…å…¨æ–‡ä¿æŒã«å¤‰æ›´ï¼‰
        pdf_filename = Path(topic).stem
        full_content = data['content']  # ãƒãƒ£ãƒ³ã‚¯åŒºåˆ‡ã‚Š "---" ã§çµåˆæ¸ˆã¿
        context_md = f"# PDF: {pdf_filename}\n\n{full_content}"

        # sourcesã«ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚‚å«ã‚ã‚‹ï¼‰
        chunks = full_content.split("\n\n---\n\n")
        sources = {
          "pdf_content": [{
            "title": pdf_filename,
            "url": topic,
            "content": data['content'][:500],  # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨
            "num_pages": data.get('num_pages', 0),
            "total_chars": data.get('total_chars', 0),
            "num_chunks": len(chunks),
            "chunks": chunks  # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿æŒ
          }]
        }

        return {
          "sources": sources,
          "context_md": context_md,
          "log": _log(state, f"[pdf] pages={data.get('num_pages')}, chars={data.get('total_chars')}, chunks={len(chunks)}")
        }
      else:
        return {"error": f"PDFå‡¦ç†ã‚¨ãƒ©ãƒ¼: {data['message']}", "log": _log(state, f"[pdf] ERROR {data['message']}")}

    # YouTubeå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
    elif input_type == "youtube":
      return {"error": "YouTubeå‡¦ç†ã¯æº–å‚™ä¸­ã§ã™ï¼ˆIssue #18ã§å®Ÿè£…äºˆå®šï¼‰", "log": _log(state, "[youtube] NOT_IMPLEMENTED")}

    # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ï¼ˆæ—¢å­˜ã®Tavilyæ¤œç´¢ï¼‰
    else:
      # JSTã®ç¾åœ¨æ—¥æ™‚ ã¨ã€€æœˆè‹±èªè¡¨è¨˜ã‚’å–å¾—
      def month_en_for(dt: datetime) -> str:
        months = ["January","February","March","April","May","June",
                  "July","August","September","October","November","December"]
        return f"{months[dt.month-1]} {dt.year}"

      now = now_jst()
      # å…ˆæœˆã¯ã€æœˆæœ«æ—¥ã®1æ—¥ã«ãªã‚‹
      prev_month_dt = now.replace(day=1) - timedelta(days=1)
      # ç›´è¿‘2ãƒ¶æœˆ
      month_labels = [month_en_for(now), month_en_for(prev_month_dt)]

      # ãƒ™ãƒ³ãƒ€ãƒ¼æ¯ã®å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³
      vendors_domains = [
            (["azure.microsoft.com","news.microsoft.com","learn.microsoft.com"],
             ["Microsoft AI updates", "Azure OpenAI updates"]),
            (["openai.com"], ["OpenAI announcements","OpenAI updates"]),
            (["blog.google","ai.googleblog.com","research.google"], ["Google AI updates", "Gemini updates"]),
            (["aws.amazon.com"], ["AWS Bedrock updates","Amazon AI updates"]),
            (["ai.meta.com"], ["Meta AI updates", "Llama updates"]),
            (["anthropic.com"], ["Anthropic Claude updates","Claude announcements"]),
        ]

      # â˜… ç›´è¿‘2ãƒ¶æœˆ Ã— å„ç¤¾ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’æ§‹æˆ
      queries: List[Dict[str, Any]] = []
      for m in month_labels:
        for domains, patterns in vendors_domains:
          for p in patterns:
            queries.append({"q": f"{p} {m}", "include_domains": domains, "time_range": "month"})

      sources = tavily_collect_context(queries, max_per_query=6, default_time_range="month")
      context_md = context_to_bullets(sources)
      return {
        "sources": sources,
        "context_md": context_md,
        "log": _log(state, f"[tavily] months={month_labels} queries={len(queries)}")
      }

  except Exception as e:
    return {"error": f"collect_info_error: {e}", "log": _log(state, f"[collect_info] EXCEPTION {e}")}

# -------------------
# Node B: é‡è¦ãƒã‚¤ãƒ³ãƒˆç”Ÿæˆ
# -------------------
@traceable(run_name="b_generate_key_points")
def generate_key_points(state: State) -> Dict:
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"
  ctx = state.get("context_md") or ""
  sources = state.get("sources") or {}

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯Map-Reduceæ–¹å¼ã§å…¨ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
  if input_type == "pdf":
    try:
      # ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = ctx.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # Map: å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºï¼ˆæœ€å¤§3å€‹ï¼‰
      chunk_points = []
      for i, chunk in enumerate(chunks[:20]):  # æœ€å¤§20ãƒãƒ£ãƒ³ã‚¯ï¼ˆLLMã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
        if not chunk.strip():
          continue

        map_prompt = [
          ("system", "ã‚ãªãŸã¯æ•™è‚²ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ä¸­å­¦ç”Ÿã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ãå†…å®¹ã‚’èª¬æ˜ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"),
          ("user",
            f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æœ€å¤§3å€‹ã€"
            f"çŸ­ã„ç®‡æ¡æ›¸ãã§æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚\n\n"
            f"[ãƒ†ã‚­ã‚¹ãƒˆéƒ¨åˆ† {i+1}]\n{chunk[:3000]}\n\n"  # ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«3000æ–‡å­—ã¾ã§
            f"ç®‡æ¡æ›¸ãå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆæœ€å¤§3å€‹ï¼‰:")
        ]

        msg = llm.invoke(map_prompt)
        points = _strip_bullets(msg.content.splitlines())[:3]
        chunk_points.extend(points)

      # Reduce: å…¨ãƒã‚¤ãƒ³ãƒˆã‚’çµ±åˆã—ã¦5ã¤ã«å‡ç¸®
      if chunk_points:
        reduce_prompt = [
          ("system", "ã‚ãªãŸã¯æ•™è‚²ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚æƒ…å ±ã‚’æ•´ç†çµ±åˆã™ã‚‹ã®ãŒå¾—æ„ã§ã™ã€‚"),
          ("user",
            f"ä»¥ä¸‹ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆãƒªã‚¹ãƒˆã‹ã‚‰ã€é‡è¤‡ã‚’å‰Šé™¤ã—ã€æœ€ã‚‚é‡è¦ãª5å€‹ã«çµã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚\n\n"
            f"[æŠ½å‡ºã•ã‚ŒãŸãƒã‚¤ãƒ³ãƒˆ]\n" + "\n".join([f"- {p}" for p in chunk_points]) + "\n\n"
            f"ã€å‡ºåŠ›å½¢å¼ã€‘\n"
            f"- å¿…ãšç®‡æ¡æ›¸ãå½¢å¼ã§å‡ºåŠ›ï¼ˆã€Œ-ã€ã¾ãŸã¯ã€Œ1.ã€ã§å§‹ã‚ã‚‹ï¼‰\n"
            f"- å¿…ãš5å€‹ã®ç®‡æ¡æ›¸ãï¼ˆ4å€‹ã‚„6å€‹ã§ã¯ãªãæ­£ç¢ºã«5å€‹ï¼‰\n"
            f"- ã€Œä»¥ä¸‹ã®5ã¤ã®ãƒã‚¤ãƒ³ãƒˆã¯...ã€ãªã©ã®å‰ç½®ãæ–‡ã¯ä¸è¦\n"
            f"- å„ãƒã‚¤ãƒ³ãƒˆã¯1è¡Œã§ç°¡æ½”ã«\n\n"
            f"ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„è¡¨ç¾ã§ã€æœ€é‡è¦ãª5å€‹ã‚’é¸ã‚“ã§ãã ã•ã„:")
        ]

        msg = llm.invoke(reduce_prompt)
        lines = msg.content.splitlines()

        # å‰ç½®ãè¡Œã‚’é™¤å¤–ï¼ˆç®‡æ¡æ›¸ãè¨˜å·ã¾ãŸã¯ç•ªå·ã§å§‹ã¾ã‚‹è¡Œã®ã¿æŠ½å‡ºï¼‰
        filtered_lines = [
          line for line in lines
          if line.strip() and (
            line.strip().startswith(('-', 'â€¢', '*', 'ãƒ»')) or
            re.match(r'^\d+\.', line.strip())
          )
        ]

        final_bullets = _strip_bullets(filtered_lines)[:5] if filtered_lines else chunk_points[:5]

        # 5å€‹æœªæº€ã®å ´åˆã¯chunk_pointsã‹ã‚‰è£œå……
        while len(final_bullets) < 5 and chunk_points:
          candidate = chunk_points.pop(0)
          if candidate not in final_bullets:  # é‡è¤‡å›é¿
            final_bullets.append(candidate)
      else:
        final_bullets = ["å†…å®¹ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"]

      return {
        "key_points": final_bullets,
        "log": _log(state, f"[key_points_map_reduce] chunks={len(chunks)}, extracted={len(chunk_points)}, final={len(final_bullets)}")
      }

    except Exception as e:
      return {"error": f"key_points_pdf_error: {e}", "log": _log(state, f"[key_points_pdf] EXCEPTION {e}")}

  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = [
      ("system", "ã‚ãªãŸã¯Solution Engineerã§ã™ã€‚ã“ã‚Œã‹ã‚‰Marpã‚¹ãƒ©ã‚¤ãƒ‰ã§AIæœ€æ–°æƒ…å ±ã‚’ç™ºè¡¨ã—ã¾ã™ã€‚"),
      ("user",
        "ä»¥ä¸‹ã®ã€Œæœ€æ–°æƒ…å ±ã‚µãƒãƒªï¼ˆå‡ºå…¸ä»˜ãï¼‰ã€ã‚’å‚è€ƒã«ã€ç™ºè¡¨ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’å„ç¤¾5å€‹ã€"
        "çŸ­ã„ç®‡æ¡æ›¸ãã§ã€‚URLã‚‚å«ã‚ã¦ãã ã•ã„ã€‚\n\n"
        f"[æœ€æ–°æƒ…å ±ã‚µãƒãƒª]\n{ctx}\n\n[ãƒˆãƒ”ãƒƒã‚¯]\n{topic}")
    ]

    try:
      msg = llm.invoke(prompt)
      bullets = _strip_bullets(msg.content.splitlines())[:5] or [msg.content.strip()]
      return {"key_points": bullets, "log": _log(state, f"[key_points] {bullets}")}
    except Exception as e:
      return {"error": f"key_points_error: {e}", "log": _log(state, f"[key_points] EXCEPTION {e}")}

# -------------------
# Node C: ç›®æ¬¡ç”Ÿæˆ
# -------------------
@traceable(run_name="c_generate_toc")
def generate_toc(state: State) -> Dict:
  topic = state.get("topic") or ""
  key_points = state.get("key_points") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  # PDFå‡¦ç†ã®å ´åˆã¯ä¸­å­¦ç”Ÿå‘ã‘ã®ç« ç«‹ã¦
  if input_type == "pdf":
    prompt = [
      ("system", "ã‚ãªãŸã¯æ•™è‚²ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚ä¸­å­¦ç”Ÿã«ã‚‚ç†è§£ã§ãã‚‹ã‚¹ãƒ©ã‚¤ãƒ‰ã®æ§‹æˆã‚’è€ƒãˆã‚‹ã®ãŒå¾—æ„ã§ã™ã€‚"),
      ("user",
       "ä»¥ä¸‹ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã€ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„5ã€œ8å€‹ã®ç« ç«‹ã¦ã‚’ JSON ã® {\"toc\": [ ... ]} å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
       "å„ç« ã‚¿ã‚¤ãƒˆãƒ«ã¯ã‚„ã•ã—ã„æ—¥æœ¬èªã§ã€‚æ§‹æˆã«æµã‚ŒãŒã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚\n\n"
       "é‡è¦ãƒã‚¤ãƒ³ãƒˆ:\n- " + "\n- ".join(key_points) + "\n\n"
       "ç« ç«‹ã¦ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§è¦ªã—ã¿ã‚„ã™ã„è¨€è‘‰ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚")
    ]
  else:
    # AIæœ€æ–°æƒ…å ±ã®å ´åˆã¯æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = [
      ("system", "ã‚ãªãŸã¯Solution Engineerã§ã™ã€‚Marpã‚¹ãƒ©ã‚¤ãƒ‰ã®æ§‹æˆï¼ˆç›®æ¬¡ï¼‰ã‚’ä½œã‚Šã¾ã™ã€‚"),
      ("user",
       "ä»¥ä¸‹ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã€5ã€œ8å€‹ã®ç« ç«‹ã¦ï¼ˆé…åˆ—ï¼‰ã‚’ JSON ã® {\"toc\": [ ... ]} å½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
       "é‡è¦ãƒã‚¤ãƒ³ãƒˆ:\n- " + "\n- ".join(key_points))
    ]

  try:
    msg = llm.invoke(prompt)
    try:
      data = json.loads(_find_json(msg.content) or msg.content)
      toc = [s.strip() for s in data.get("toc", []) if s.strip()]
    except Exception as e:
      toc = _strip_bullets(msg.content.splitlines())
      toc = toc[:8] or ["ã¯ã˜ã‚ã«", "èƒŒæ™¯", "å®Ÿè£…æ‰‹é †", "è©•ä¾¡ã¨æ”¹å–„", "å…¬é–‹ãƒ»é‹ç”¨", "ã¾ã¨ã‚"]
    return {"toc": toc, "error": "", "log": _log(state, f"[toc] {toc}")}
  except Exception as e:
    return {"error": f"toc_error: {e}", "log": _log(state, f"[toc] EXCEPTION {e}")}

# -------------------
# Node D: ã‚¹ãƒ©ã‚¤ãƒ‰æœ¬æ–‡ï¼ˆSlidevï¼‰ç”Ÿæˆ
# -------------------
@traceable(run_name="d_generate_slide_slidev")
def write_slides_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆï¼ˆå…¨6ç¤¾å¯¾å¿œ / PDFå¯¾å¿œï¼‰"""
  sources = state.get("sources") or {}
  topic = state.get("topic") or "AIæœ€æ–°æƒ…å ±"
  context_md = state.get("context_md") or ""
  key_points = state.get("key_points") or []
  toc = state.get("toc") or []

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥
  input_type = detect_input_type(topic)

  try:
    # PDFå‡¦ç†ã®å ´åˆã¯æ±ç”¨çš„ãªã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    if input_type == "pdf":
      # â˜…å…¨ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰è¦ç´„ã‚’ä½œæˆã—ã¦ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆ
      pdf_data = sources.get("pdf_content", [{}])[0]
      chunks = pdf_data.get("chunks", [])

      if not chunks:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: context_mdã‹ã‚‰åˆ†å‰²
        full_content = context_md.replace("# PDF: ", "").split("\n\n", 1)[-1]
        chunks = full_content.split("\n\n---\n\n")

      # PDFã®å†…å®¹ã‹ã‚‰LLMã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆ
      title_prompt = [
        ("system", "ã‚ãªãŸã¯æ•™è‚²ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚PDFã®å†…å®¹ã‹ã‚‰é­…åŠ›çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚"),
        ("user",
         f"ä»¥ä¸‹ã®PDFå†…å®¹ã‹ã‚‰ã€ä¸­å­¦ç”Ÿå‘ã‘ã‚¹ãƒ©ã‚¤ãƒ‰ã«ãµã•ã‚ã—ã„é­…åŠ›çš„ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’1è¡Œã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
         f"ã€PDFå†’é ­ã€‘\n{chunks[0][:2000] if chunks else 'å†…å®¹ãªã—'}\n\n"
         f"ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘\n" + "\n".join([f"- {kp}" for kp in key_points[:3]]) + "\n\n"
         f"ã€è¦ä»¶ã€‘\n"
         f"- ä¸­å­¦ç”ŸãŒèˆˆå‘³ã‚’æŒã¤ã‚¿ã‚¤ãƒˆãƒ«\n"
         f"- ç°¡æ½”ã§ã‚ã‹ã‚Šã‚„ã™ã„ï¼ˆ15æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰\n"
         f"- çµµæ–‡å­—ã¯ä¸è¦\n"
         f"- ä¾‹: ã€ŒAIã®ä»•çµ„ã¿ã¨æœªæ¥ã€ã€Œåœ°çƒæ¸©æš–åŒ–ã‚’å­¦ã¼ã†ã€\n\n"
         f"ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„:")
      ]

      try:
        title_msg = llm.invoke(title_prompt)
        ja_title = title_msg.content.strip().replace('"', '').replace("'", '')
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
        if len(ja_title) > 30:
          ja_title = ja_title[:30] + "..."
      except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ç”Ÿæˆ
        pdf_filename = Path(topic).stem if topic.endswith('.pdf') else "PDFè³‡æ–™"
        # UUIDã‚’é™¤å»
        if '_' in pdf_filename:
          pdf_filename = pdf_filename.split('_', 1)[1] if len(pdf_filename.split('_', 1)) > 1 else pdf_filename
        ja_title = f"{pdf_filename}"

      # ãƒãƒ£ãƒ³ã‚¯ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆè¦ç´„ãªã—ãƒ»ã‚³ã‚¹ãƒˆå‰Šæ¸›ç‰ˆï¼‰
      chunk_texts = []
      total_chars = 0
      for i, chunk in enumerate(chunks[:20]):
        if not chunk.strip():
          continue

        # å„ãƒãƒ£ãƒ³ã‚¯ã®å…ˆé ­1500æ–‡å­—ã‚’ä½¿ç”¨
        chunk_preview = chunk[:1500]
        chunk_texts.append(f"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³{i+1}\n{chunk_preview}")
        total_chars += len(chunk_preview)

        # åˆè¨ˆ15,000æ–‡å­—ã¾ã§ä½¿ç”¨ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆã®ä¸Šé™ï¼‰
        if total_chars > 15000:
          break

      # å…¨ãƒãƒ£ãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
      full_summary = "\n\n".join(chunk_texts)

      # LLMã§Slidevãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’ç”Ÿæˆï¼ˆãƒãƒ£ãƒ³ã‚¯æŠœç²‹ç‰ˆã‚’ä½¿ç”¨ï¼‰
      prompt = [
        ("system",
         "ã‚ãªãŸã¯æ•™è‚²ã®ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã§ã™ã€‚PDFã®å†…å®¹ã‚’ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹Slidevã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¾ã™ã€‚å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ç›®æ¬¡ã«åŸºã¥ã„ã¦çŸ­ã„ç‰©èªã‚„ä¼šè©±å½¢å¼ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚1æšã®ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã¯ä¼ãˆãŸã„ãƒã‚¤ãƒ³ãƒˆã‚’ã²ã¨ã¤ã ã‘è¼‰ã›ã€çµµæ–‡å­—ã‚‚æ´»ç”¨ã—ã¦å­ã©ã‚‚ã®èˆˆå‘³ã‚’å¼•ã„ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯å¯èƒ½ãªé™ã‚Šé¿ã‘ã€å„ªã—ã„å£èª¿ã§è©±ã—ã‹ã‘ã‚‹ã‚ˆã†ã«èª¬æ˜ã—ã¾ã—ã‚‡ã†ã€‚"),
        ("user",
         f"ä»¥ä¸‹ã®PDFå†…å®¹ï¼ˆæŠœç²‹ï¼‰ã‹ã‚‰ã€ä¸­å­¦ç”Ÿå‘ã‘ã®ã‚ã‹ã‚Šã‚„ã™ã„ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚\n\n"
         f"ã€PDFå†…å®¹ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥æŠœç²‹ï¼‰ã€‘\n{full_summary}\n\n"
         f"ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘\n" + "\n".join([f"- {kp}" for kp in key_points[:5]]) + "\n\n"
         f"ã€ç›®æ¬¡ã€‘\n" + "\n".join([f"- {t}" for t in toc[:8]]) + "\n\n"
         f"ã€è¦ä»¶ã€‘\n"
         f"- Slidevå½¢å¼ï¼ˆYAMLãƒ•ãƒ­ãƒ³ãƒˆãƒã‚¿ãƒ¼ + Markdownï¼‰\n"
         f"- theme: apple-basic ã‚’ä½¿ç”¨\n"
         f"- ã‚¿ã‚¤ãƒˆãƒ«ã‚¹ãƒ©ã‚¤ãƒ‰: # {ja_title}\n"
         f"- Agendaã‚¹ãƒ©ã‚¤ãƒ‰: ç›®æ¬¡ã‚’ç®‡æ¡æ›¸ãã§è¡¨ç¤ºï¼ˆ<v-clicks>ã¯ä½¿ã‚ãªã„ï¼‰\n"
         f"- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³: ç›®æ¬¡ã«åŸºã¥ã„ã¦5-8ã‚¹ãƒ©ã‚¤ãƒ‰ä½œæˆ\n"
         f"- å„ã‚¹ãƒ©ã‚¤ãƒ‰ã¯ç°¡æ½”ã«ï¼ˆ1ã‚¹ãƒ©ã‚¤ãƒ‰1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰\n"
         f"- ä¸­å­¦ç”Ÿã§ã‚‚ã‚ã‹ã‚‹è¨€è‘‰ã§èª¬æ˜\n"
         f"- çµµæ–‡å­—ã‚’æ´»ç”¨ã—ã¦è¦–è¦šçš„ã«\n"
         f"- ã§ãã‚Œã°çŸ­ã„ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚„ã€å…ˆç”Ÿã¨ç”Ÿå¾’ã®ä¼šè©±å½¢å¼ã‚‚ä½¿ã£ã¦ãã ã•ã„\n"
         f"- ã¾ã¨ã‚ã‚¹ãƒ©ã‚¤ãƒ‰: ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã€‘ã§ç¤ºã—ãŸ5å€‹ã™ã¹ã¦ã‚’ç®‡æ¡æ›¸ãã§è¡¨ç¤ºã—ã¦ãã ã•ã„\n"
         f"- PDFå…¨ä½“ã®æµã‚Œã‚’åæ˜ ã—ãŸè«–ç†çš„ãªæ§‹æˆã«ã—ã¦ãã ã•ã„\n\n"
         f"å‡ºåŠ›ã¯Slidevãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã¿ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ä¸è¦ï¼‰:")
      ]

      msg = llm.invoke(prompt)
      slide_md = msg.content.strip()

      # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ãŒã‚ã‚Œã°é™¤å»
      slide_md = _strip_whole_code_fence(slide_md)

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev_pdf] generated ({len(slide_md)} chars) from {len(chunk_texts)} chunks (cost-optimized, no LLM summary)")
      }

    # AIæœ€æ–°æƒ…å ±ï¼ˆTavilyï¼‰ã®å ´åˆã¯æ—¢å­˜ã®ãƒãƒ«ãƒãƒ™ãƒ³ãƒ€ãƒ¼ç”Ÿæˆ
    else:
      ja_title = f"{month_ja()} AIæœ€æ–°æƒ…å ±ã¾ã¨ã‚"
      slide_md = _generate_multi_vendor_slides_integrated(
        topic=ja_title,
        sources=sources,
        mvp_version="AI Industry Report 2025"
      )

      return {
        "slide_md": slide_md,
        "title": ja_title,
        "error": "",
        "log": _log(state, f"[slides_slidev] generated ({len(slide_md)} chars, 6 vendors)")
      }

  except Exception as e:
    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ç”Ÿæˆ
    fallback_md = f"""---
theme: apple-basic
highlighter: shiki
class: text-center
---

# ğŸš€ {ja_title}
## ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ

<div class="pt-12">
  <span class="px-2 py-1 rounded" style="background: #ef4444; color: white;">
    Error: {str(e)}
  </span>
</div>

---

## âš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°

ã‚¹ãƒ©ã‚¤ãƒ‰ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

- æ¤œç´¢çµæœã®å–å¾—ã«å¤±æ•—ã—ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„

"""
    return {
      "slide_md": fallback_md,
      "title": ja_title,
      "error": f"slides_slidev_error: {e}",
      "log": _log(state, f"[slides_slidev] EXCEPTION {e} - using fallback")
    }

# -------------------
# Node E: è©•ä¾¡
# -------------------
MAX_ATTEMPTS = 3

# Slidevç”¨è©•ä¾¡ãƒãƒ¼ãƒ‰
@traceable(run_name="e_evaluate_slides_slidev")
def evaluate_slides_slidev(state: State) -> Dict:
  """Slidevã‚¹ãƒ©ã‚¤ãƒ‰ã®å“è³ªè©•ä¾¡ï¼ˆPDF/AIæƒ…å ±å¯¾å¿œï¼‰"""
  if state.get("error"):
    return {}
  slide_md = state.get("slide_md") or ""
  toc = state.get("toc") or []
  topic = state.get("topic") or ""

  # å…¥åŠ›ã‚¿ã‚¤ãƒ—ã‚’åˆ¤åˆ¥ã—ã¦PDFç‰¹æœ‰ã®è©•ä¾¡åŸºæº–ã‚’è¿½åŠ 
  input_type = detect_input_type(topic)

  if input_type == "pdf":
    # PDFå‘ã‘è©•ä¾¡åŸºæº–ï¼ˆä¸­å­¦ç”Ÿå‘ã‘è§£èª¬ï¼‰
    eval_guide = (
        "è©•ä¾¡è¦³ç‚¹ã¨é‡ã¿:\n"
        "- structure(0.20): ã‚¹ãƒ©ã‚¤ãƒ‰ã®æµã‚Œã€ç« ç«‹ã¦ã€1ã‚¹ãƒ©ã‚¤ãƒ‰1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n"
        "- comprehensiveness(0.25): PDFå…¨ä½“ã®é‡è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹ã‹ï¼ˆç¶²ç¾…æ€§ï¼‰\n"
        "- clarity(0.25): ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚‹èª¬æ˜ã€é›£ã—ã„ç”¨èªã®è¨€ã„æ›ãˆ\n"
        "- readability(0.15): ç°¡æ½”æ˜ç­ã€è¦–èªæ€§ï¼ˆç®‡æ¡æ›¸ãã®ç²’åº¦ã€çµµæ–‡å­—æ´»ç”¨ï¼‰\n"
        "- engagement(0.15): èˆˆå‘³ã‚’å¼•ãå·¥å¤«ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€ä¼šè©±å½¢å¼ã€è¦–è¦šè¦ç´ ï¼‰\n"
        "åˆæ ¼: score >= 8.0\n"
        "\n"
        "ã€é‡è¦ã€‘\n"
        "- PDFã®æœ€åˆã®ãƒšãƒ¼ã‚¸ã ã‘ã§ãªãã€å…¨ä½“ã®æµã‚Œã‚’åæ˜ ã—ã¦ã„ã‚‹ã“ã¨\n"
        "- å°‚é–€ç”¨èªã¯ä¸­å­¦ç”Ÿã«ã‚‚ã‚ã‹ã‚‹è¨€è‘‰ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨\n"
        "- çµµæ–‡å­—ã‚„è¦–è¦šè¦ç´ ã§è¦–è¦šçš„ã«ç†è§£ã—ã‚„ã™ã„ã“ã¨\n"
    )
  else:
    # AIæƒ…å ±å‘ã‘è©•ä¾¡åŸºæº–ï¼ˆæ—¢å­˜ï¼‰
    eval_guide = (
        "è©•ä¾¡è¦³ç‚¹ã¨é‡ã¿:\n"
        "- structure(0.20): ã‚¹ãƒ©ã‚¤ãƒ‰ã®æµã‚Œã€ç« ç«‹ã¦ã€1ã‚¹ãƒ©ã‚¤ãƒ‰1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n"
        "- practicality(0.25): å®Ÿå‹™ã«ä½¿ãˆã‚‹å…·ä½“æ€§ï¼ˆæ‰‹é †ã€å…·ä½“ä¾‹ã€æ³¨æ„ç‚¹ï¼‰\n"
        "- accuracy(0.25): äº‹å®Ÿãƒ»ç”¨èªã®æ­£ç¢ºã•\n"
        "- readability(0.15): ç°¡æ½”æ˜ç­ã€è¦–èªæ€§ï¼ˆç®‡æ¡æ›¸ãã®ç²’åº¦ï¼‰\n"
        "- conciseness(0.15): å†—é•·æ€§ã®å°‘ãªã•\n"
        "åˆæ ¼: score >= 8.0\n"
    )
  prompt = [
        ("system",
         "ã‚ãªãŸã¯Cloud Solution Architectã§ã™ã€‚ä»¥ä¸‹ã®Slidevã‚¹ãƒ©ã‚¤ãƒ‰Markdownã‚’ã€"
         "æŒ‡å®šã®è¦³ç‚¹ãƒ»é‡ã¿ã§å³å¯†ã«æ¡ç‚¹ã—ã¾ã™ã€‚å‡ºåŠ›ã¯JSONã®ã¿ã€‚"),
        ("user",
         f"Topic: {topic}\nTOC: {json.dumps(toc, ensure_ascii=False)}\n\n"
         "Slides (Slidev Markdown):\n<<<SLIDES\n" + slide_md + "\nSLIDES\n\n"
         "Evaluation Guide:\n<<<EVAL\n" + eval_guide + "\nEVAL\n\n"
         "Return strictly this JSON schema:\n"
         "{\n"
         "  \"score\": number,\n"
         "  \"subscores\": {\"structure\": number, \"practicality\": number, \"accuracy\": number, \"readability\": number, \"conciseness\": number},\n"
         "  \"reasons\": {\"structure\": string, \"practicality\": string, \"accuracy\": string, \"readability\": string, \"conciseness\": string},\n"
         "  \"suggestions\": [string],\n"
         "  \"risk_flags\": [string],\n"
         "  \"pass\": boolean,\n"
         "  \"feedback\": string\n"
         "}"
        )]
  try:
    msg = llm.invoke(prompt)
    raw = msg.content or ""
    js = _find_json(raw) or raw
    data = json.loads(js)

    score = float(data.get("score", 0.0))
    subscores = data.get("subscores") or {}
    reasons = data.get("reasons") or {}
    suggestions = data.get("suggestions") or []
    risk_flags = data.get("risk_flags") or []
    passed = bool(data.get("pass", score >= 8.0))
    feedback = str(data.get("feedback", "")).strip()
    attempts = (state.get("attempts") or 0) + 1

    return {
      "score": score,
      "subscores": subscores,
      "reasons": reasons,
      "suggestions": suggestions,
      "risk_flags": risk_flags,
      "passed": passed,
      "feedback": feedback,
      "attempts": attempts,
      "log": _log(state, f"[evaluate_slidev] score={score:.2f} pass={passed} attempts={attempts}")
    }
  except Exception as e:
    return {"error": f"eval_error: {e}", "log": _log(state, f"[evaluate_slidev] EXCEPTION {e}")}

def route_after_eval_slidev(state: State) -> str:
    """è©•ä¾¡çµæœã«åŸºã¥ã„ã¦ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯å®Œäº†ã‚’åˆ¤å®š"""
    if (state.get("attempts") or 0) >= MAX_ATTEMPTS:
        return "ok"
    return "ok" if state.get("passed") else "retry"

# -------------------
# Node F: ä¿å­˜ & Slidevãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
# -------------------
@traceable(run_name="f_save_and_render_slidev")
def save_and_render_slidev(state: State) -> Dict:
  """Slidevå½¢å¼ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’ä¿å­˜ã—ã¦PDFç”Ÿæˆ"""
  if state.get("error"):
    return {}

  slide_md = state.get("slide_md") or ""
  title = state.get("title") or "AIã‚¹ãƒ©ã‚¤ãƒ‰"

  # ã‚¹ãƒ©ã‚¤ãƒ‰å†…å®¹ãŒç©ºã®å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
  if not slide_md.strip():
    return {
      "error": "slide_md is empty",
      "log": _log(state, "[save_slidev] ERROR: slide_md is empty")
    }

  # ã‚¹ãƒ©ã‚¤ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«åã®è‹±èªè¡¨è¨˜ã‚’ç”Ÿæˆ
  slug_prompt = [
    ("system", "You create concise English slugs for filenames."),
    ("user",
     "Convert the following Japanese title into a short English filename base (<=6 words). "
     "Only letters and spaces; no punctuation or numbers.\n\n"
     f"Title: {title}")
  ]

  try:
    emsg = llm.invoke(slug_prompt)
    file_stem = _slugify_en(emsg.content.strip()) or _slugify_en(title)
  except Exception:
    file_stem = _slugify_en(title) or "ai-latest-info"

  slide_dir = Path(__file__).parent.parent.parent / "data" / "slides"
  slide_dir.mkdir(parents=True, exist_ok=True)
  slide_md_path = slide_dir / f"{file_stem}_slidev.md"
  slide_md_path.write_text(slide_md, encoding="utf-8")

  # Slidev PDFç”Ÿæˆ
  slidev = shutil.which("slidev")
  out_path = str(slide_md_path)

  if slidev and SLIDE_FORMAT == "pdf":
    pdf_file = slide_dir / f"{file_stem}_slidev.pdf"
    try:
      subprocess.run(
        ["slidev", "export", str(slide_md_path),
         "--output", str(pdf_file),
         "--format", "pdf",
         "--timeout", "60000"],  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        check=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        timeout=90  # ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
      )
      out_path = str(pdf_file)
      log_msg = f"[slidev] generated PDF -> {pdf_file.name}"
    except subprocess.TimeoutExpired:
      log_msg = f"[slidev] PDF generation timeout (90s) - MD saved at {slide_md_path.name}"
    except subprocess.CalledProcessError as e:
      error_msg = e.stderr.decode() if e.stderr else str(e)
      log_msg = f"[slidev] export failed: {error_msg[:100]} - MD saved"
  else:
    if not slidev:
      log_msg = "[slidev] slidev-cli not found â€“ skipped rendering (left .md)."
    else:
      log_msg = f"[slidev] rendering skipped (SLIDE_FORMAT={SLIDE_FORMAT}, only pdf supported)."

  return {
    "slide_path": out_path,
    "log": _log(state, log_msg)
  }

# -------------------
# ã‚°ãƒ©ãƒ•æ§‹ç¯‰
# -------------------
graph_builder = StateGraph(State)
graph_builder.add_node("collect_info", collect_info)
graph_builder.add_node("generate_key_points", generate_key_points)
graph_builder.add_node("generate_toc", generate_toc)
graph_builder.add_node("write_slides_slidev", write_slides_slidev)
graph_builder.add_node("save_and_render_slidev", save_and_render_slidev)
graph_builder.add_node("evaluate_slides_slidev", evaluate_slides_slidev)

# ã‚¨ãƒƒã‚¸å®šç¾©ï¼ˆSlidevãƒ•ãƒ­ãƒ¼ with è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼‰
graph_builder.add_edge(START, "collect_info")
graph_builder.add_edge("collect_info", "generate_key_points")
graph_builder.add_edge("generate_key_points", "generate_toc")
graph_builder.add_edge("generate_toc", "write_slides_slidev")
graph_builder.add_edge("write_slides_slidev", "evaluate_slides_slidev")

# è©•ä¾¡ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€å¤§3å›ãƒªãƒˆãƒ©ã‚¤ï¼‰
graph_builder.add_conditional_edges(
  "evaluate_slides_slidev",
  route_after_eval_slidev,
  {"retry": "generate_key_points", "ok": "save_and_render_slidev"}
)

graph_builder.add_edge("save_and_render_slidev", END)

graph = graph_builder.compile()

# -------------------
# å®Ÿè¡Œ
# -------------------
if __name__ == "__main__":
  print("LangSmith Tracing:", os.getenv("LANGCHAIN_TRACING_V2"),
      "| Project:", os.getenv("LANGCHAIN_PROJECT"))
  init: State = {
    "topic": "LangGraph Ã— LangSmith Ã— OpenAI Ã— Tavilyã§ä½œã‚‹ï¼šæœ€æ–°AIå‹•å‘ã‚¹ãƒ©ã‚¤ãƒ‰",
    "key_points": [], "toc": [], "slide_md": "",
    "score": 0.0,
    "subscores": {}, "reasons": {},
    "suggestions": [], "risk_flags": [],
    "passed": False, "feedback": "",
    "title": "", "slide_path": "",
    "attempts": 0, "error": "", "log": [],
    "context_md": "", "sources": {}
  }

  config: RunnableConfig = {
    "run_name": "tavily_marp_agent",
    "tags": ["marp", "langgraph", "langsmith", "openai", "tavily"],
    "metadata": {"env": "dev", "date": datetime.now(timezone.utc).isoformat()},
    "recursive_limit": 60,
  }
  out = graph.invoke(init, config=config)

  print("\n=== RESULT ===")
  if out.get("error"):
    print("ERROR:", out["error"])
  else:
    print("Title    :", out.get("title"))
    print("Slide    :", out.get("slide_path"))
    # print("Score    :", out.get("score"))
    # print("Passed   :", out.get("passed"))
  print("\n=== LOGS ===")
  for line in out.get("log", []):
    print(line)